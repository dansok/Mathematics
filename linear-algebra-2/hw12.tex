\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm, mathrsfs}
\addtolength{\evensidemargin}{-.5in}
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\textwidth}{0.8in}
\addtolength{\textheight}{0.8in}
\addtolength{\topmargin}{-.4in}
\newtheoremstyle{quest}{\topsep}{\topsep}{}{}{\bfseries}{}{ }{\thmname{#1}\thmnote{ #3}.}
\theoremstyle{quest}
\newtheorem*{definition}{Definition}
\newtheorem*{theorem}{Theorem}
\newtheorem*{question}{Question}
\newtheorem*{exercise}{Exercise}
\newtheorem*{challengeproblem}{Challenge Problem}
\newcommand{\name}{%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% put your name here, so we know who to give credit to %%
Dan Sokolsky
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\hw}{%%%%%%%%%%%%%%%%%%%%
%% and which homework assignment is it? %%%%%%%%%
%% put the correct number below              %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
12
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\vspace{-50pt}
\Huge \name
\\\vspace{20pt}
\huge Linear Algebra II\hfill Homework \hw}
\author{}
\date{}
\pagestyle{myheadings}
\markright{\name\hfill Homework \hw\qquad\hfill}

%% If you want to define a new command, you can do it like this:
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

%% If you want to use a function like ''sin'' or ''cos'', you can do it like this
%% (we probably won't have much use for this)
% \DeclareMathOperator{\sin}{sin}   %% just an example (it's already defined)


\begin{document}
\maketitle

\begin{question}[1]
\end{question}
\begin{proof}
$\mathbf{(a)}$ Let $b = \begin{pmatrix}
1 \\
1
\end{pmatrix}$. Then $Y = span \{e_1\}, Z = span \{b\}$, and $e_2 = -e_1 + b$. Thus $Pe_2 = -e_1 = - \begin{pmatrix}
1 \\
0
\end{pmatrix} $.
\\$\mathbf{(b)}$ Let $I_{s \times n} = \begin{pmatrix}
I_s & 0
\end{pmatrix} $ be $s \times n$. Let $\tilde{I}_{t \times n} = \begin{pmatrix}
0 & I_t
\end{pmatrix} $ be $t \times n$. Then,
$$P = \begin{pmatrix}
y_1 & \ldots & y_s & z_1 & \ldots & z_t
\end{pmatrix}  = Y \cdot I_{s \times n}  + Z \cdot \tilde{I}_{t \times n} $$
\\$\mathbf{(c)}$ Let $\beta_Y = \{\hat{y}_1, \ldots, \hat{y}_s\}$, $\beta_Z = \{\hat{z}_1, \ldots, \hat{z}_s\}$ form orthonormal bases for $Y, Z$, respectively. Let
$$A = \begin{pmatrix}
\hat{y}_1 & \ldots & \hat{y}_s & 0 & \ldots & 0
\end{pmatrix}, B = \begin{pmatrix}
\hat{z}_1 & \ldots & \hat{z}_t & 0 & \ldots & 0
\end{pmatrix} $$
and observe that for any $v \in X$ --
$$Av = \sum_{i=1}^s \hat{y}_i v_i \in Y,\ \ \ \ Bv = \sum_{i=1}^t \hat{z}_i v_i \in Z$$
We see that in particular, $A, B$ are onto $Y, Z$, respectively. Additionally,
\\$A^TA = \begin{pmatrix}
I_s & 0 \\
0 & 0
\end{pmatrix} $, $B^TB = \begin{pmatrix}
I_t & 0 \\
0 & 0
\end{pmatrix} $. That is, $A, B$ are isometric. I.e., $A, B$ preserve the norms of vectors in their row spaces. Furthermore, for all $w \in X$, $w = w_{\perp} + w_N$, where $w_{\perp} \in N_A^{\perp}$, and $w_N \in N_A$. So
$$Aw = A(w_{\perp} + w_N) = Aw_{\perp}\ \ \ \ \ \ \ \ \ \ \ (12.1)$$
Likewise, for all $v \in X$, $v = v_{\perp} + v_N$, where $v_{\perp} \in N_B^{\perp}$, and $v_N \in N_B$. So
$$Bv = B(v_{\perp} + v_N) = Bv_{\perp}\ \ \ \ \ \ \ \ \ \ \ (12.2)$$
Now consider the matrix $M = A^TB$. By $(12.1)$, and $(12.2)$, in the following discussion suffices to consider only $v \in N_B^{\perp}, w \in N_A^{\perp}, \|v\| = 1 = \|w\|$ (and indeed, by question $\mathbf{(1)(a)}$ in homework $11$, the maximizers are achieved in these respective subspaces). Then denoting $z= Bv, y = Aw$, we have $\|z\| = \|Bv\| = \|v\| = 1 = \|w\| = \|Aw\| = \|y\|$. Thus,
$$\sigma = \|M\| = \sup_{\hat{v} \in N_B^{\perp}} \|M\hat{v}\| = \sup_{\hat{v} \in N_B^{\perp},\hat{w} \in N_A^{\perp}} \langle M\hat{v}, \hat{w} \rangle = \sup_{\hat{v} \in N_B^{\perp},\hat{w} \in N_A^{\perp}} \langle A^TB \hat{v}, \hat{w} \rangle = $$ $$= \sup_{\hat{v} \in N_B^{\perp},\hat{w} \in N_A^{\perp}} \langle B \hat{v}, A \hat{w} \rangle = \sup_{\hat{z} \in Z, \hat{y} \in Y} \langle \hat{z}, \hat{y} \rangle = \cos \theta(Y, Z)$$
Therefore, $M = A^TB$ is the desired matrix.
\end{proof}

\begin{question}[2]
Let $u = u_1$ be a unit vector. Let the orthogonal projection onto $span\{u\}$ be $P_u = uu^T$. Then $P_u^{\perp} = I - P_u = I - uu^T$. We have
$$P_u A = uu^T(I + uv^T) = uu^T + uv^T = P_u + A - I$$
and
$$P_u^{\perp} A = (I - uu^T)(I + uv^T) = I - uu^T + uv^T - uu^Tuv^T = I - uu^T + uv^T - uv^T = I - uu^T = P_u^{\perp}$$
and
$$(P_u)^2 = (uu^T)^2 = u(u^Tu)u^T = u \cdot 1 \cdot u^T = uu^T$$
and
$$(P_u^{\perp})^2 = (I - uu^T)^2 = I - 2uu^T + (uu^T)^2 = I - 2uu^T + uu^T = I - uu^T = P_u^{\perp}$$
and
$$P_u \cdot P_u^{\perp} = uu^T(I - uu^T) = uu^T - uu^T = 0$$
We see that this setup satisfies the resolution of the identity described on page $110$.
Note That $P_u^{\perp}$ acts as an identity on $span\{u\}^{\perp}$. Let $u_2 \ in span\{u\}^{\perp}$ be a unit vector. Define $P_{u_2} = u_2 u_2^T$. Define $P_{u_2}^{\perp} = P_{u}^{\perp} - P_{u_2} = I - P_u - P_{u_2}$. Now $P_{u_2}^{\perp}$ is the orthogonal projection onto $span\{u, u_2\}^{\perp}$. Continuing in this way, we get $I = \sum_{i=1}^n P_{u_i} = \sum_{i=1}^n u_i u_i^T$. Since this decomposition is a resolution of the identity, it follows that $\{u_i\}_{1 \le i \le n}$ forms a basis for $X$.
\end{question}

\begin{question}[3]
\end{question}
\begin{proof}
$\mathbf{(a)}$ since $\|uv^T\| < 1$, the geometric series gives --
$$(I + uv^T)^{-1} = \sum_{j=0}^\infty (-uv^T)^j = I + u \Big[\sum_{j=1}^\infty (-1)^j(v^Tu)^{j-1}\Big] v^T$$
$$= I - u \Big[\sum_{j=1}^\infty (-v^Tu)^{j-1}\Big] v^T = I - u \Big[\sum_{j=0}^\infty (-v^Tu)^j\Big] v^T = I - \dfrac{uv^T}{1 + v^Tu}$$
\\$\mathbf{(b)}$ We have
$$I = AA^{-1} = (I - uv^T)A^{-1} = A^{-1} - uv^TA^{-1}$$
Now,
$$v^T = v^T \cdot I = v^T(A^{-1} - uv^TA^{-1}) = v^T A^{-1} - v^Tuv^TA^{-1} = (1 - v^Tu) \cdot v^TA^{-1}$$
$$\iff v^TA^{-1} = \dfrac{v^T}{1 - v^Tu}$$
$$\iff I = A^{-1} - uv^TA^{-1} = A^{-1} - u \cdot \dfrac{v^T}{1 - v^Tu} = A^{-1} - \dfrac{uv^T}{1 - v^Tu}$$
$$\iff A^{-1} = I + \dfrac{uv^T}{1 - v^Tu}$$
\end{proof}

\begin{question}[4]
\end{question}
\begin{proof}
$\mathbf{(a)}$ Suppose $G$ is isometric. Then for $v \in N_G^{\perp}$, $\|G v \| = \|v\|$. Additionally, $G$ preserves the inner product on $N_G^{\perp}$ -- Let $u,v \in N_G^{\perp}$. Then,
$$\|u\|^2 -2 \langle Gu, Gv \rangle + \|v\|^2 = \|Gu\|^2 -2 \langle Gu, Gv \rangle + \|Gv\|^2 = \|Gu - Gv \|^2$$
$$= \|G (u-v)\|^2 = \|u - v\|^2  = \|u\|^2 -2 \langle u, v \rangle + \|v\|^2$$
$$\iff \langle Gu, Gv \rangle = \langle u, v \rangle$$
Then for any $u, v \in N_G^{\perp}$, we have --
$$\langle u, v \rangle = \langle Gu, Gv \rangle = \langle u, G^TG v \rangle \iff 0 = \langle u, G^TGv \rangle - \langle u, v \rangle  = \langle u, G^TGv - v \rangle $$
In particular, for $u = G^TG v - v$, we have --
$$\|G^TGv - v\|^2 = \langle G^TGv - v, G^TGv - v \rangle  = 0$$
$$\iff G^TGv - v = 0 \iff G^TGv = v$$
Since $v \in N_G^{\perp}$ is arbitrary, we have that $G^TG$ acts as an identity on $N_G^{\perp}$.
\\Now, for $v \in N_G$, we have
$$G^TGv= G^T \cdot 0 = 0$$
Let $x \in X = N_G^{\perp} \oplus N_G$. Then $x$ can be written uniquely: $x = u +v$, for $u \in N_G^{\perp},\\ v \in N_G$. We have --
$$G^TGx = G^TG(u+v) = G^TGu + G^TGv = u + 0 = u$$
Obviously $N_G^{\perp} \perp N_G$ by definition, whence we have that $G^TG$ is the orthogonal projection of $X$ onto $N_G^{\perp}$.
\\$\mathbf{(b)}$ Let $k = \dim N_G^{\perp}$. By $\mathbf{(a)}$, $G^TG = \begin{pmatrix}
I_k & 0 \\
0 & 0
\end{pmatrix} $. That is, taking all the singular values to be the positive square roots, we have $\sigma_i  = 1$. Since this gives a diagonal matrix $\Sigma$, it follows that $\Sigma$ is diagonalized by the identity matrix, hence the matrix of right singular vectors is $V = I_{k \times n} = \begin{pmatrix}
I_k & 0
\end{pmatrix}$. I.e., $v_i = e_i$; whence this gives
$$u_i = \sigma_i \cdot u_i = G v_i = G e_i = g_i$$
for $1 \le i \le k$. As indeed is expected since the first $k$ columns of $G$ must be orthonormal if $G^TG$ is to be an orthogonal projection. Thus $G = \sum_{i=1}^k \sigma_i u_i v_i^T = \sum_{i=1}^k g_i e_i^T$ is the desired reduced SVD. In matrix form --
$$G = \begin{pmatrix}
g_1 & \ldots & g_k
\end{pmatrix} \cdot I_k \cdot I_{k \times n}$$
\end{proof}
\end{document}
