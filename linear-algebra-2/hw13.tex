\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm, mathrsfs}
\addtolength{\evensidemargin}{-.5in}
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\textwidth}{0.8in}
\addtolength{\textheight}{0.8in}
\addtolength{\topmargin}{-.4in}
\newtheoremstyle{quest}{\topsep}{\topsep}{}{}{\bfseries}{}{ }{\thmname{#1}\thmnote{ #3}.}
\theoremstyle{quest}
\newtheorem*{definition}{Definition}
\newtheorem*{theorem}{Theorem}
\newtheorem*{question}{Question}
\newtheorem*{exercise}{Exercise}
\newtheorem*{challengeproblem}{Challenge Problem}
\newcommand{\name}{%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% put your name here, so we know who to give credit to %%
Dan Sokolsky
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\hw}{%%%%%%%%%%%%%%%%%%%%
%% and which homework assignment is it? %%%%%%%%%
%% put the correct number below              %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
13
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\vspace{-50pt}
\Huge \name
\\\vspace{20pt}
\huge Linear Algebra II\hfill Homework \hw}
\author{}
\date{}
\pagestyle{myheadings}
\markright{\name\hfill Homework \hw\qquad\hfill}

%% If you want to define a new command, you can do it like this:
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

%% If you want to use a function like ''sin'' or ''cos'', you can do it like this
%% (we probably won't have much use for this)
% \DeclareMathOperator{\sin}{sin}   %% just an example (it's already defined)


\begin{document}
\maketitle

\begin{question}[1]
\end{question}
\begin{proof}
The claim is false in general. Let $X = Y \oplus Z$. Let $P$ be an oblique projection of $X$ onto $Y$ along $Z$. That is, for $X \ni x = y + z$, $Px = P(y+z) = y$. First, we claim $N_P = Z$ -- To that end, $z \in Z \implies Pz = P(0 + z) = 0 \iff z \in N_P$. Conversely, let $x \in N_P$. Then $x$ can be written uniquely: $x = y + z$, $y \in Y, z \in Z$. Thus $0 = Px = P(y+z) = y$. That is, $x = z \in Z$. Thus $Z = N_P$, as claimed.
\end{proof}

\begin{question}[2]
Let $u = u_1$ be a unit vector. Let the orthogonal projection onto $span\{u\}$ be $P_u = uu^T$. Then $P_u^{\perp} = I - P_u = I - uu^T$. We have
$$P_u A = uu^T(I + uv^T) = uu^T + uv^T = P_u + A - I$$
and
$$P_u^{\perp} A = (I - uu^T)(I + uv^T) = I - uu^T + uv^T - uu^Tuv^T = I - uu^T + uv^T - uv^T = I - uu^T = P_u^{\perp}$$
and
$$(P_u)^2 = (uu^T)^2 = u(u^Tu)u^T = u \cdot 1 \cdot u^T = uu^T$$
and
$$(P_u^{\perp})^2 = (I - uu^T)^2 = I - 2uu^T + (uu^T)^2 = I - 2uu^T + uu^T = I - uu^T = P_u^{\perp}$$
and
$$P_u \cdot P_u^{\perp} = uu^T(I - uu^T) = uu^T - uu^T = 0$$
We see that this setup satisfies the resolution of the identity described on page $110$.
Note That $P_u^{\perp}$ acts as an identity on $span\{u\}^{\perp}$. Let $u_2 \ in span\{u\}^{\perp}$ be a unit vector. Define $P_{u_2} = u_2 u_2^T$. Define $P_{u_2}^{\perp} = P_{u}^{\perp} - P_{u_2} = I - P_u - P_{u_2}$. Now $P_{u_2}^{\perp}$ is the orthogonal projection onto $span\{u, u_2\}^{\perp}$. Continuing in this way, we get $I = \sum_{i=1}^n P_{u_i} = \sum_{i=1}^n u_i u_i^T$. Since this decomposition is a resolution of the identity, it follows that $\{u_i\}_{1 \le i \le n}$ forms a basis for $X$.
\end{question}

\begin{question}[3]
\end{question}
\begin{proof}
$\mathbf{(a)}$ since $\|uv^T\| < 1$, the geometric series gives --
$$(I + uv^T)^{-1} = \sum_{j=0}^\infty (-uv^T)^j = I + u \Big[\sum_{j=1}^\infty (-1)^j(v^Tu)^{j-1}\Big] v^T$$
$$= I - u \Big[\sum_{j=1}^\infty (-v^Tu)^{j-1}\Big] v^T = I - u \Big[\sum_{j=0}^\infty (-v^Tu)^j\Big] v^T = I - \dfrac{uv^T}{1 + v^Tu}$$
\\$\mathbf{(b)}$ We have
$$I = AA^{-1} = (I - uv^T)A^{-1} = A^{-1} - uv^TA^{-1}$$
Now,
$$v^T = v^T \cdot I = v^T(A^{-1} - uv^TA^{-1}) = v^T A^{-1} - v^Tuv^TA^{-1} = (1 - v^Tu) \cdot v^TA^{-1}$$
$$\iff v^TA^{-1} = \dfrac{v^T}{1 - v^Tu}$$
$$\iff I = A^{-1} - uv^TA^{-1} = A^{-1} - u \cdot \dfrac{v^T}{1 - v^Tu} = A^{-1} - \dfrac{uv^T}{1 - v^Tu}$$
$$\iff A^{-1} = I + \dfrac{uv^T}{1 - v^Tu}$$
\end{proof}

\begin{question}[4]
\end{question}
\begin{proof}
$\mathbf{(a)}$ Suppose $G$ is isometric. Then for $v \in N_G^{\perp}$, $\|G v \| = \|v\|$. Additionally, $G$ preserves the inner product on $N_G^{\perp}$ -- Let $u,v \in N_G^{\perp}$. Then,
$$\|u\|^2 -2 \langle Gu, Gv \rangle + \|v\|^2 = \|Gu\|^2 -2 \langle Gu, Gv \rangle + \|Gv\|^2 = \|Gu - Gv \|^2$$
$$= \|G (u-v)\|^2 = \|u - v\|^2  = \|u\|^2 -2 \langle u, v \rangle + \|v\|^2$$
$$\iff \langle Gu, Gv \rangle = \langle u, v \rangle$$
Then for any $u, v \in N_G^{\perp}$, we have --
$$\langle u, v \rangle = \langle Gu, Gv \rangle = \langle u, G^TG v \rangle \iff 0 = \langle u, G^TGv \rangle - \langle u, v \rangle  = \langle u, G^TGv - v \rangle $$
In particular, for $u = G^TG v - v$, we have --
$$\|G^TGv - v\|^2 = \langle G^TGv - v, G^TGv - v \rangle  = 0$$
$$\iff G^TGv - v = 0 \iff G^TGv = v$$
Since $v \in N_G^{\perp}$ is arbitrary, we have that $G^TG$ acts as an identity on $N_G^{\perp}$.
\\Now, for $v \in N_G$, we have
$$G^TGv= G^T \cdot 0 = 0$$
Let $x \in X = N_G^{\perp} \oplus N_G$. Then $x$ can be written uniquely: $x = u +v$, for $u \in N_G^{\perp},\\ v \in N_G$. We have --
$$G^TGx = G^TG(u+v) = G^TGu + G^TGv = u + 0 = u$$
Obviously $N_G^{\perp} \perp N_G$ by definition, whence we have that $G^TG$ is the orthogonal projection of $X$ onto $N_G^{\perp}$.
\\$\mathbf{(b)}$ Let $k = \dim N_G^{\perp}$. By $\mathbf{(a)}$, $G^TG = \begin{pmatrix}
I_k & 0 \\
0 & 0
\end{pmatrix} $. That is, taking all the singular values to be the positive square roots, we have $\sigma_i  = 1$. Since this gives a diagonal matrix $\Sigma$, it follows that $\Sigma$ is diagonalized by the identity matrix, hence the matrix of right singular vectors is $V = I_{k \times n} = \begin{pmatrix}
I_k & 0
\end{pmatrix}$. I.e., $v_i = e_i$; whence this gives
$$u_i = \sigma_i \cdot u_i = G v_i = G e_i = g_i$$
for $1 \le i \le k$. As indeed is expected since the first $k$ columns of $G$ must be orthonormal if $G^TG$ is to be an orthogonal projection. Thus $G = \sum_{i=1}^k \sigma_i u_i v_i^T = \sum_{i=1}^k g_i e_i^T$ is the desired reduced SVD. In matrix form --
$$G = \begin{pmatrix}
g_1 & \ldots & g_k
\end{pmatrix} \cdot I_k \cdot I_{k \times n}$$
\end{proof}
\end{document}
