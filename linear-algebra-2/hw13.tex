\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm, mathrsfs}
\addtolength{\evensidemargin}{-.5in}
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\textwidth}{0.8in}
\addtolength{\textheight}{0.8in}
\addtolength{\topmargin}{-.4in}
\newtheoremstyle{quest}{\topsep}{\topsep}{}{}{\bfseries}{}{ }{\thmname{#1}\thmnote{ #3}.}
\theoremstyle{quest}
\newtheorem*{definition}{Definition}
\newtheorem*{theorem}{Theorem}
\newtheorem*{question}{Question}
\newtheorem*{exercise}{Exercise}
\newtheorem*{challengeproblem}{Challenge Problem}
\newcommand{\name}{%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% put your name here, so we know who to give credit to %%
Dan Sokolsky
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\hw}{%%%%%%%%%%%%%%%%%%%%
%% and which homework assignment is it? %%%%%%%%%
%% put the correct number below              %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
13
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\vspace{-50pt}
\Huge \name
\\\vspace{20pt}
\huge Linear Algebra II\hfill Homework \hw}
\author{}
\date{}
\pagestyle{myheadings}
\markright{\name\hfill Homework \hw\qquad\hfill}

%% If you want to define a new command, you can do it like this:
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

%% If you want to use a function like ''sin'' or ''cos'', you can do it like this
%% (we probably won't have much use for this)
% \DeclareMathOperator{\sin}{sin}   %% just an example (it's already defined)


\begin{document}
\maketitle

\begin{question}[1]
\end{question}
\begin{proof}
The claim is false in general. Let $X = Y \oplus Z$. Let $P$ be an oblique projection of $X$ onto $Y$ along $Z$. That is, for $X \ni x = y + z$, $Px = P(y+z) = y$. First, we claim $N_P = Z$ -- To that end, $z \in Z \implies Pz = P(0 + z) = 0 \iff z \in N_P$. Conversely, let $x \in N_P$. Then $x$ can be written uniquely: $x = y + z$, $y \in Y, z \in Z$. Thus $0 = Px = P(y+z) = y$. That is, $x = z \in Z$. Thus $Z = N_P$, as claimed. Having established this, we proceed with a counter example. Let $X = \mathbb{R}^2$. Let $Z = span\{\begin{pmatrix}
1 \\
0
\end{pmatrix} \} = span\{e_1\}$. Let $Y = span\{\begin{pmatrix}
1 \\
1
\end{pmatrix}$. Let $P$ be the projection onto $Y$ along $Z$. By our claim, $N_P = Z$. Thus $N_P^{\perp} = span\{\begin{pmatrix}
0 \\
1
\end{pmatrix}\} = span\{e_2\}$. Then
$$e_2 = \begin{pmatrix}
0 \\
1
\end{pmatrix} = \begin{pmatrix}
1 \\
1
\end{pmatrix} - \begin{pmatrix}
1 \\
0
\end{pmatrix} = \begin{pmatrix}
1 \\
1
\end{pmatrix} - e_1$$
Now $Pe_2 = \begin{pmatrix}
1 \\
1
\end{pmatrix}$ but
$$\|e_1\| = 1 \ne 2 = 1^2 + 1^2 = \|\begin{pmatrix}
1 \\
1
\end{pmatrix}\| = \|Pe_1\|$$
\end{proof}

\begin{question}[2]
Let $f(x) = x^TBx$.
\\$\mathbf{(a)}$ We take advantage of the fact that for real vectors $x, y \in \mathbb{R}^n$, $\langle x, y \rangle = \langle y, x \rangle$. In particular, $u^TAv = \langle u, Av \rangle = \langle Av, u \rangle = v^TA^Tu$
$$f(x + ty) = (x + ty)^TB(x + ty) = x^TBx + t(y^TBx + x^TBy)+t^2y^TBy$$
By our definition and comment above,
$$df_x(y) = y^TBx + x^TBy = x^TB^Ty + x^TBy = x^T(B + B^T)y = \langle (B + B^T)x, y \rangle$$
so that our linear operator $df_x = (B + B^T)x$, which is clearly a column vector.
\\$\mathbf{(b)}$ Let $H$ be symmetric. Let $R_H(x) = \dfrac{x^THx}{x^Tx} = \dfrac{q(x)}{p(x)}$. Note that
$$R(cx) = \dfrac{cx^TH(cx)}{cx^T(cx)} = \dfrac{c^2}{c^2} \cdot \dfrac{x^THx}{x^Tx} = \dfrac{x^THx}{x^Tx} = R(x)$$
Thus suffices to study the situation for $\|x\|^2 = x^Tx = 1$, and note that $R$ must take it's minimum at some point on the unit sphere. Call that point $f$. Then
$$R(f + tg) = \dfrac{f^THf + 2g^THf + t^2f^THf}{f^Tf + 2tg^Tf + t^2g^Tg} = \dfrac{q(t)}{p(t)}$$
Since $R$ achieves it's minimum at $f$, $R(f + tg)$ achieves it's minimum at $t=0$. Thus
$$0 = \dfrac{d}{dt}R(f + tg)|_{t=0} = R' = \dfrac{q'p - p'q}{p^2}$$
Now $\|f\| = p = 1$, and denoting $\lambda = R_H(f) = \min R$, we have
$$0 = R' = q' - \lambda p'\ \ \ \ \ \ \ \ \ \ \ \ (13.1)$$
Finally,
$$q' = \dfrac{d}{dt}q(f + tg)|_{t=0} = 2g^THf$$
$$p' = \dfrac{d}{dt}q(f + tg)|_{t=0} = 2g^Tf$$
We get readily from $(13.1)$
$$0 = 2g^THf - 2\lambda g^Tf = 2g^T(Hf - \lambda f)\ \ \ \ \ \ \ \ \ \ \ \ (13.2)$$
$(13.2)$ is true for all $g \in \mathbb{R}^n$. A vector orthogonal to all other vectors is the $0$ vector. It follows that $Hf - \lambda f = 0 \iff Hf = \lambda f$. I.e., $(\lambda, f)$ is an eigenpair of $H$.
\end{question}

\begin{question}[3]
\end{question}
\begin{proof}
$\mathbf{(a)}$ Let $q(X) = X^TAX$. Then
$$dq_X(Y) = \lim_{t \rightarrow 0} \dfrac{q(X + tY) - q(X)}{t} = \lim_{t \rightarrow 0} \dfrac{(X + tY)^TA(X + tY) - X^TAX}{t} =$$
$$\lim_{t \rightarrow 0} \dfrac{X^TAX + t(X^TAY + Y^TAX) + t^2(Y^TAY) - X^TAX}{t} = X^TAY + Y^TAX$$
which is our compact format, since $A, X, X^T, Y, Y^T$ are all $n \times n$ matrices.
\\$\mathbf{(b)}$ Let $r(X) = AX^2$. Then
$$dr_X(Y) = \lim_{t \rightarrow 0} \dfrac{r(X + tY) - r(X)}{t} = \lim_{t \rightarrow 0} \dfrac{A(X+tY)^2 - AX^2}{t} =$$
$$\lim_{t \rightarrow 0} \dfrac{AX^2 + tA(XY + YX) + t^2AY^2 - AX^2}{t} = A(XY + YX)$$
Identifying each matrix $M \in M_{n \times n}(\mathbb{R})$ with a vector $vec(M) \in \mathbb{R}^{n^2}$, we describe the matrix action by the action of $dr_X$ on the $n^2$ basis elements $e_i e_j^T$ --
$$dr_X(e_i e_j^T) = A(X e_i e_j^T + e_i e_j^T X) \in M_{n \times n}(\mathbb{R}) \simeq \mathbb{R}^{n^2}$$
this yields an $n^2 \times n^2$ matrix representation of the linear operator $dr_X$.
\end{proof}

\begin{question}[4]
\end{question}
\begin{proof}
$\mathbf{(a)}$ Suppose $A$ is not invertible. Then $AA^*$ is a self-adjoint, positive semi-definite. We can find a self-adjoint, positive semi-definite $R$, such that $AA^* = R^2$, and by the spectral representation theorem $R = W\Sigma W^*$, whence $R^2 = W\Sigma^2 W^*$. The problem now is to find a unitary matrix $U$, such that $A = RU$. Let us tackle this issue by introducing $H_{\sigma} = AA^* + \sigma I$. Then $H_{\sigma}$ is self-adjoint, strictly positive definite, and we have
$$H_{\sigma} = AA^* + \sigma I = W\Sigma^2 W^* + \sigma WW^* = W(\Sigma^2 + \sigma I)W^* = R_{\sigma}^2\ \ \ \ \ \ \ \ \ \ (13.3)$$
$$H_{\sigma} = AA^* + \sigma I = QS^2Q^* + \sigma I = Q(S^2 + \sigma I)Q^*\ \ \ \ \ \ \ \ \ \ (13.4)$$
This motivates the definition $A_{\sigma} = Q(S + \sqrt{\sigma} I)Q^*$. For $S + \sqrt{\sigma} I$ is a diagonal matrix, and therefore --
$$A_{\sigma}A_{\sigma}^* = Q(S + \sqrt{\sigma} I)Q^* Q(S + \sqrt{\sigma} I)Q^* = Q(S + \sqrt{\sigma} I)^2Q^*$$
$$= Q(S^2 + \sigma I) + (2\sqrt{\sigma}S)Q^* = P_{\sigma}^2 \rightarrow R_{\sigma}^2 \rightarrow R^2\ \ \ \ \ \ \ \ \ \ (13.5)$$
as $\sigma \rightarrow 0$, and $P_{\sigma}$ is positive definite. Define $U_{\sigma} = P_{\sigma}^{-1}A_{\sigma}$. Then $A_{\sigma} = P_{\sigma}U_{\sigma}$, and $A = \lim_{\sigma \rightarrow 0} A_{\sigma} = \lim_{\sigma \rightarrow 0} P_{\sigma}U_{\sigma} = RU$. As before,
$$U_{\sigma} U_{\sigma}^* = P_{\sigma}^{-1}A_{\sigma} A_{\sigma}^* P_{\sigma}^{-1} = P_{\sigma}^{-1}P_{\sigma}^2 P_{\sigma}^{-1} = I$$
I.e., $U_{\sigma}$ is unitary. Remains to show $U = \lim_{\sigma \rightarrow 0}U_{\sigma}$ is itself unitary. To this end, we claim that the set of unitary operators, $\mathscr{U}$ is compact, whence the limit of any sequence of unitary matrices is itself unitary. We show $\mathscr{U}$ is closed and bounded --
\\$(1)$ $\mathscr{U}$ is bounded -- $M \in \mathscr{U} \implies \|M\| = 1 \implies M$ is bounded.
\\$(2)$ $\mathscr{U}$ is closed as the inverse image of the closed singleton $\{I\}$ under the continuous mapping $A \mapsto AA^*$.
Therefore, $U$ is unitary as well, as required. We have the desired decomposition $A = RU$.
\\$\mathbf{(b)}$ Let $A = W\Sigma V^* = (W \Sigma W^*) \cdot (WV^*) = R \cdot U$, where $R = W \Sigma W^*, U = WV^*$. Obviously $U$ is unitary as a product of unitary matrices, and $R$ is positive semi-definite since $\Sigma$ is positive semi-definite (all singular values $\sigma_i \ge 0$), and positive (semi-)definite matrices are closed under $M \mapsto QMQ^*$ (in the strictly positive case we also require that $Q$ should have full rank which it does in our case as an orthogonal matrix). Finally, if $A$ is invertible, then $R$ is strictly positive, as the entries on the diagonal of $\Sigma$ are positive, whence $\Sigma$ is positive.
\end{proof}
\end{document}
