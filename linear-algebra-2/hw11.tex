\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm, mathrsfs}
\addtolength{\evensidemargin}{-.5in}
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\textwidth}{0.8in}
\addtolength{\textheight}{0.8in}
\addtolength{\topmargin}{-.4in}
\newtheoremstyle{quest}{\topsep}{\topsep}{}{}{\bfseries}{}{ }{\thmname{#1}\thmnote{ #3}.}
\theoremstyle{quest}
\newtheorem*{definition}{Definition}
\newtheorem*{theorem}{Theorem}
\newtheorem*{question}{Question}
\newtheorem*{exercise}{Exercise}
\newtheorem*{challengeproblem}{Challenge Problem}
\newcommand{\name}{%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% put your name here, so we know who to give credit to %%
Dan Sokolsky
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\hw}{%%%%%%%%%%%%%%%%%%%%
%% and which homework assignment is it? %%%%%%%%%
%% put the correct number below              %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
11
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\vspace{-50pt}
\Huge \name
\\\vspace{20pt}
\huge Linear Algebra II\hfill Homework \hw}
\author{}
\date{}
\pagestyle{myheadings}
\markright{\name\hfill Homework \hw\qquad\hfill}

%% If you want to define a new command, you can do it like this:
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

%% If you want to use a function like ''sin'' or ''cos'', you can do it like this
%% (we probably won't have much use for this)
% \DeclareMathOperator{\sin}{sin}   %% just an example (it's already defined)


\begin{document}
\maketitle

\begin{question}[1]
\end{question}
\begin{proof}
$\mathbf{(a)}$ Since $A \ne 0$, there exists $w \in \mathbb{R}^{n}$ such that $Aw \ne 0$. Thus
\\$\|Av\| = \|A\| \ge \|Aw\| > 0$; whence $Av \ne 0$. That is $v \not\in N_A \iff v \in N_A^{\perp}$ since $N_A \oplus N_A^{\perp} = \mathbb{R}^n$.
\\$\mathbf{(b)}$ $\|v\| = 1$, so
$$\sigma = \|A\| = \|Av\| = \langle Av, \frac{Av}{\|Av\|} \rangle = \langle Av, \frac{\sigma u}{\sigma} \rangle = \langle Av, u \rangle = \langle v, A^T u \rangle$$
Now,
$$\sup_{\|x\|=1, \|w\| = 1}\langle A^Tx, w \rangle = \|A^T\| = \|A\| = \sigma = \langle A^Tu, v  \rangle = \sup_{\|w\|=1} \langle A^Tu, w \rangle = \|A^Tu\|$$
That is, $A^T$ achieves it's maximum for $x = u,\ w=v$. That is,
$$\langle A^Tu, v  \rangle = \sigma = \|A^T\| = \|A^Tu\| = \|A^Tu\| \cdot 1 = \|A^Tu\| \cdot \|v\|$$
$$\iff A^Tu = k \cdot v \iff v = \dfrac{A^Tu}{k}$$
since by Cauchy-Schwartz, equality is achieved $\iff$ the two vectors are scalar multiples of one another. Now,
$$\|A^Tu\| = \langle A^Tu, v\rangle = \langle A^Tu, \dfrac{A^Tu}{k}\rangle = \dfrac{1}{k} \langle A^Tu, A^Tu \rangle = \dfrac{1}{k} \|A^Tu\|^2$$
$$\iff k = \|A^Tu\| = \sigma$$
That is, $A^Tu = \sigma v$.

\end{proof}

\begin{question}[2]
\end{question}
\begin{proof}
$\mathbf{(a)}$ We iterate step 1. Find a unit vector $v_2 \in \mathbb{R}^n$ such that $\|A_2\| = \|A_2v_2\|$.
\\Let $\sigma_2 u_2 = A_2 v_2,\ \sigma_2 = \|A_2\|$ so that $u_2 \in \mathbb{R}^m$ is a unit vector.
\\$\mathbf{(b)}$ By question $(1)(a)$, $v_2 \in N_{A_2}^{\perp}$. Define $A_3 = A_2 - \sigma_2 u_2 v_2^T$. Then
$$A_3 v_2 = A_2 v_2 - \sigma_2 u_2 v_2^Tv_2 = A_2 v_2 - \sigma_2 u_2 \|v_2\|^2 = A_2 v_2 - \sigma_2 u_2 = 0$$
That is, $v_2 \in N_{A_3}$.
\\$\mathbf{(c)}$ Since $v_1 \in N_{A_2}$ and $v_2 \in N_{A_2}^{\perp}$, it follows that $v_1 \perp v_2$, or $\langle v_1, v_2 \rangle = 0$.
\\$\mathbf{(d)}$ We have $A_i = A_{i-1} - \sigma_{i-1} u_{i-1} v_{i-1}^T = A - \Sigma_{j=1}^{i-1} \sigma_j u_j v_j^T$.
\end{proof}

\begin{question}[3]
\end{question}
\begin{proof}
WLOG, suppose $A = USV^T, S\ k \times k$ is a tall matrix in reduced SVD form. Let $S$ be $k \times k$. $U, V$ are orthogonal. By definition $2$ of homework $9$, we need to show that $U, V^T$ are isometric. We will show that orthogonal matrices are isometric. Question $(4)(b)$ tells us that $V^T$ is isometric as well.
We show $U$ is isometric, and the argument for $V$ is identical. Note that $U^TU = I_k$, and we have for all $x$ --
$$\|Ux\|^2 = \langle Ux, Ux \rangle = \langle x, U^TUx \rangle = \langle x, Ix \rangle =\langle x, x \rangle = \|x\|^2$$
$\iff \|Ux\| = \|x\|$. That is, $U$ is isometric. Likewise, $V$ is isometric. By question $(4)(b)$, $V^T$ is isometric. Finally, a quick note on the inclusion of the row and column spaces as mentioned in definition $2$ --
$$0 \ne Sx \in Range(S) \implies \|USx\| = \|Sx\| > 0 \iff USx \ne 0$$
Thus $Sx \not\in N_U \iff Sx \in N_U^{\perp}$. We have $columnspace(S) \subseteq rowspace(U)$.
\\$rowspace(S) \subseteq columnspace(V^T)$ is trivial. This shows that $U, V$ were properly selected (but of course this is obvious by construction of the SVD).
\end{proof}

\begin{question}[4]
\end{question}
\begin{proof}
$\mathbf{(a)}$ Suppose $G$ is isometric. Then for $v \in N_G^{\perp}$, $\|G v \| = \|v\|$. Additionally, $G$ preserves the inner product on $N_G^{\perp}$ -- Let $u,v \in N_G^{\perp}$. Then,
$$\|u\|^2 -2 \langle Gu, Gv \rangle + \|v\|^2 = \|Gu\|^2 -2 \langle Gu, Gv \rangle + \|Gv\|^2 = \|Gu - Gv \|^2$$
$$= \|G (u-v)\|^2 = \|u - v\|^2  = \|u\|^2 -2 \langle u, v \rangle + \|v\|^2$$
$$\iff \langle Gu, Gv \rangle = \langle u, v \rangle\ \ \ \ \ \ \ \ \ \ \ \ (11.1)$$
Then for any $u, v \in N_G^{\perp}$, we have --
$$\langle u, v \rangle = \langle Gu, Gv \rangle = \langle u, G^TG v \rangle \iff 0 = \langle u, G^TGv \rangle - \langle u, v \rangle  = \langle u, G^TGv - v \rangle $$
In particular, for $u = G^TG v - v$, we have --
$$\|G^TGv - v\|^2 = \langle G^TGv - v, G^TGv - v \rangle  = 0$$
$$\iff G^TGv - v = 0 \iff G^TGv = v$$
since $v \in N_G^{\perp}$ is arbitrary. That is, $G^TG$ acts as an identity on $N_G^{\perp}$.
\\Now, for $v \in N_G$, we have
$$G^TGv= G^T \cdot 0 = 0$$
Let $x \in X = N_G^{\perp} \oplus N_G$. Then $x$ can be written uniquely: $x = u +v$, for $u \in N_G^{\perp},\\ v \in N_G$. We have --
$$G^TGx = G^TG(u+v) = G^TGu + G^TGv = u + 0 = u$$
Obviously $N_G^{\perp} \perp N_G$ by definition, whence we have that $G^TG$ is the orthogonal projection of $X$ onto $N_G^{\perp}$.
\\$\mathbf{(b)}$ $X = N_G^{\perp} \oplus N_G$. So,
$$u \in N_G^{\perp} \iff u \not \in N_G \iff Gu \ne 0 \iff N_{G \restriction_{N_G^{\perp}}} = \{0\}$$
$\iff G \restriction_{N_G^{\perp}}$ is bijective and hence invertible. Additionally, for $0 \ne u \in N_G^{\perp}$, we showed in $\mathbf{(a)}$ that
$$G^TGu = u \ne 0 \iff 0 \ne Gu \not\in N_{G^T} \iff 0 \ne Gu \in N_{G^T}^{\perp}$$
(note that this also shows that $G \restriction_{N_G^{\perp}}$ is bijective). Since $\dim N_G^{\perp} = \dim N_{G^T}^{\perp}$ as the row rank, and column rank of $G$, respectively, it follows that $G \restriction_{N_G^{\perp}}: N_G^{\perp} \rightarrow N_{G^T}^{\perp}$ is a bijection. Thus, for any $v \in N_{G^T}^{\perp}$, there exists $u \in N_G^{\perp}$ such that $Gu = v$. Since $G$ is isometric, and $G^TG$ acts as an identity on $N_G^{\perp}$, we have --
$$\|G^Tv\| = \|G^TGu\| = \|u\| = \|Gu\| = \|v\|$$
\end{proof}

\begin{question}[5]
\end{question}
\begin{proof}
Suppose $\dim V = n, \dim U = m$.
\\$\mathbf{(a)}$ See $(11.1)$.
\\$\mathbf{(b)}$ By question $(4)(a)$, it follows that $G^TG$ is an orthogonal projection (WLOG suppose the first k entries on the diagonal are ones. That is, suppose the first $k$ column vectors of $G$ are the orthonormal ones). That is, taking all the singular values to be the positive square roots, we have $\sigma_i  = 1$. Since this gives a diagonal matrix $\Sigma$, it follows that $\Sigma$ is diagonalized by the identity matrix, hence the matrix of right singular vectors is $V = I_{k \times n}$. I.e., $v_i = e_i$; whence this gives
$$u_i = \sigma_i \cdot u_i = G v_i = G e_i = g_i$$
for $1 \le i \le k$. As indeed is expected since the first $k$ columns of $G$ must be orthonormal if $G^TG$ is to be an orthogonal projection. Thus $G = \sum_{i=1}^k \sigma_i u_i v_i^T = \sum_{i=1}^k g_i e_i^T$ is the desired reduced SVD. In matrix form --
$$G = \begin{pmatrix}
g_1 & \ldots & g_k
\end{pmatrix} \cdot I_k \cdot I_{k \times n}$$
\\$\mathbf{(c)}$ Since $\{g_1, \ldots, g_k\}$ spans $N_G^{\perp}$ (and is orthonormal), it follows that $\{g_{k+1}, \ldots, g_m\}$ spans $N_G$. Let us orthogonalize the latter set using Gram-Schmidt to give an orthonormal basis to $N_G$. For $k+1 \le j \le m$,
$$\hat{g}_j = g_j - \sum_{i=1}^{j-1}\dfrac{\langle g_i, g_j \rangle}{\|g_i\|^2}g_i$$
That is, the full SVD in matrix form is --
$$G = \begin{pmatrix}
g_1 & \ldots & g_k & \hat{g}_{k+1} & \ldots & \hat{g}_m
\end{pmatrix} \cdot I_{m \times n} \cdot I_n = U \Sigma V^T$$
where $\Sigma$ is diagonal, and $U, V$ are both orthogonal.
\end{proof}

\end{document}
