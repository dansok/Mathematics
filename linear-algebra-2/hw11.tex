\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm, mathrsfs}
\addtolength{\evensidemargin}{-.5in}
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\textwidth}{0.8in}
\addtolength{\textheight}{0.8in}
\addtolength{\topmargin}{-.4in}
\newtheoremstyle{quest}{\topsep}{\topsep}{}{}{\bfseries}{}{ }{\thmname{#1}\thmnote{ #3}.}
\theoremstyle{quest}
\newtheorem*{definition}{Definition}
\newtheorem*{theorem}{Theorem}
\newtheorem*{question}{Question}
\newtheorem*{exercise}{Exercise}
\newtheorem*{challengeproblem}{Challenge Problem}
\newcommand{\name}{%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% put your name here, so we know who to give credit to %%
Dan Sokolsky
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\hw}{%%%%%%%%%%%%%%%%%%%%
%% and which homework assignment is it? %%%%%%%%%
%% put the correct number below              %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
11
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\vspace{-50pt}
\Huge \name
\\\vspace{20pt}
\huge Linear Algebra II\hfill Homework \hw}
\author{}
\date{}
\pagestyle{myheadings}
\markright{\name\hfill Homework \hw\qquad\hfill}

%% If you want to define a new command, you can do it like this:
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

%% If you want to use a function like ''sin'' or ''cos'', you can do it like this
%% (we probably won't have much use for this)
% \DeclareMathOperator{\sin}{sin}   %% just an example (it's already defined)


\begin{document}
\maketitle

\begin{question}[1]
\end{question}
\begin{proof}
$\mathbf{(a)}$ Since $A \ne 0$, there exists $w \in \mathbb{R}^{n}$ such that $Aw \ne 0$. Thus
\\$\|Av\| = \|A\| \ge \|Aw\| > 0$; whence $Av \ne 0$. That is $v \not\in N_A \iff v \in N_A^{\perp}$ since $N_A \oplus N_A^{\perp} = \mathbb{R}^n$.
\\$\mathbf{(b)}$ $\|v\| = 1$, so
$$\sigma = \|A\| = \|Av\| = \langle Av, \frac{Av}{\|Av\|} \rangle = \langle Av, \frac{\sigma u}{\sigma} \rangle = \langle Av, u \rangle = \langle v, A^T u \rangle$$
Now,
$$\sup_{\|x\|=1, \|w\| = 1}\langle A^Tx, w \rangle = \|A^T\| = \|A\| = \sigma = \langle A^Tu, v  \rangle = \sup_{\|w\|=1} \langle A^Tu, w \rangle = \|A^Tu\|$$
That is, $A^T$ achieves it's maximum for $x = u,\ w=v$. That is,
$$\langle A^Tu, v  \rangle = \sigma = \|A^T\| = \|A^Tu\| = \|A^Tu\| \cdot 1 = \|A^Tu\| \cdot \|v\|$$
$$\iff A^Tu = k \cdot v \iff v = \dfrac{A^Tu}{k}$$
since by Cauchy-Schwartz, equality is achieved $\iff$ the two vectors are scalar multiples of one another. Now,
$$\|A^Tu\| = \langle A^Tu, v\rangle = \langle A^Tu, \dfrac{A^Tu}{k}\rangle = \dfrac{1}{k} \langle A^Tu, A^Tu \rangle = \dfrac{1}{k} \|A^Tu\|^2$$
$$\iff k = \|A^Tu\| = \sigma$$
That is, $A^Tu = \sigma v$.

\end{proof}

\begin{question}[2]
\end{question}
\begin{proof}
$\mathbf{(a)}$ We iterate step 1. Find a unit vector $v_2 \in \mathbb{R}^n$ such that $\|A_2\| = \|A_2v_2\|$.
\\Let $\sigma_2 u_2 = A_2 v_2,\ \sigma_2 = \|A_2\|$ so that $u_2 \in \mathbb{R}^m$ is a unit vector.
\\$\mathbf{(b)}$ By question $(1)(a)$, $v_2 \in N_{A_2}^{\perp}$. Define $A_3 = A_2 - \sigma_2 u_2 v_2^T$. Then
$$A_3 v_2 = A_2 v_2 - \sigma_2 u_2 v_2^Tv_2 = A_2 v_2 - \sigma_2 u_2 \|v_2\|^2 = A_2 v_2 - \sigma_2 u_2 = 0$$
That is, $v_2 \in N_{A_3}$.
\\$\mathbf{(c)}$ Since $v_1 \in N_{A_2}$ and $v_2 \in N_{A_2}^{\perp}$, it follows that $v_1 \perp v_2$, or $\langle v_1, v_2 \rangle = 0$.
\\$\mathbf{(d)}$ We have $A_i = A_{i-1} - \sigma_{i-1} u_{i-1} v_{i-1}^T = A - \Sigma_{j=1}^{i-1} \sigma_j u_j v_j^T$.
\end{proof}

\begin{question}[3]
\end{question}
\begin{proof}
$\mathbf{(a)}$ Let $A_1 = U_1 \Sigma_1 V_1^T,\ A_2 = U_2 \Sigma_2 V_2^T$. Define $U = \begin{pmatrix}
U_1 & 0 \\
0 & U_2
\end{pmatrix}$, $V = \begin{pmatrix}
V_1 & 0 \\
0 & V_2
\end{pmatrix}$, $\Sigma = \begin{pmatrix}
\Sigma_1 & 0 \\
0 & \Sigma_2
\end{pmatrix}$. Then $U, V$ are orthogonal matrices since $U_i, V_i$ are orthogonal matrices, and $\Sigma$ is diagonal since $\Sigma_1, \Sigma_2$ are diagonal. Then
\\$A = \begin{pmatrix}
A_1 & 0 \\
0 & A_2
\end{pmatrix} = \begin{pmatrix}
U_1 \Sigma_1 V_1^T & 0 \\
0 & U_2 \Sigma_2 V_2^T 
\end{pmatrix} = U \Sigma V^T$.
\\$\mathbf{(b)}$ Let $\beta = \{x_1, \ldots, x_m, x_{m+1}, \ldots, x_n\}$ be any basis for $X$ such that $\{x_1, \ldots, x_m\}$ spans $X_1$, and $\{x_{m+1}, \ldots, x_n\}$ spans $X_2$. Since $AX_1 \subseteq X_1$ and $AX_2 \subseteq X_2$, $A \restriction_{X_1} : X_1 \rightarrow X_1$ and $A \restriction_{X_2} : X_2 \rightarrow X_2$, and we can write w.r.t $\beta$ --
$$A =  \begin{pmatrix}
A \restriction_{X_1} & 0 \\
0 & A \restriction_{X_2}
\end{pmatrix} = \begin{pmatrix}
U_1 \Sigma_1 V_1^T & 0 \\
0 & U_2 \Sigma_2 V_2^T
\end{pmatrix} = U \cdot \Sigma \cdot V^T$$
as in $\mathbf{(a)}$.
\\$\mathbf{(c)}$ Let $A = \begin{pmatrix}
f_1 & \ldots & f_m & f_{m+1} & \ldots & f_n
\end{pmatrix}$. Let $F = span \{f_1, \ldots, f_m\}$, $G = span \{f_{m+1}, \ldots, f_n\}$, $F \oplus G = X$. For $f \in F, g \in G$, $f \cdot g$ is an odd function, so $\langle f, g \rangle = \int_{-1}^{-1} f \cdot g\ dx = 0$. Thus $F \perp G$. Thus
$$A^TA = \begin{pmatrix}
f_1 \\ \vdots \\ f_m \\ f_{m+1} \\ \vdots \\ f_n
\end{pmatrix} \cdot \begin{pmatrix}
f_1 & \ldots & f_m & f_{m+1} & \ldots & f_n
\end{pmatrix} =$$
$$\begin{pmatrix}
\langle f_1, f_1 \rangle & \ldots & \langle f_1, f_m \rangle & 0 & \ldots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
\langle f_m, f_1 \rangle & \ldots & \langle f_m, f_m \rangle & 0 & \ldots & 0 \\
0 & \ldots & 0 & \langle f_{m+1}, f_{m+1} \rangle & \ldots & \langle f_{m+1}, f_n \rangle 
 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \ldots & 0 & \langle f_n, f_{m+1} \rangle & \ldots & \langle f_n, f_n \rangle 
\end{pmatrix} = \begin{pmatrix}
B_F & 0 \\
0 & B_G
\end{pmatrix} $$
Thus $B_F, B_G$ and $A^TA$ are symmetric, and are therefore diagonalizable by orthogonal eigenvectors. I.e.,
$$A^TA = \begin{pmatrix}
B_F & 0 \\
0 & B_G
\end{pmatrix} = \begin{pmatrix}
Q^T_F \Lambda_F Q_F & 0 \\
0 & Q^T_G \Lambda_G Q_G
\end{pmatrix} =$$
$$\begin{pmatrix}
Q^T_F & 0 \\
0 & Q^T_G
\end{pmatrix}  \cdot \begin{pmatrix}
\Lambda_F & 0 \\
0 & \Lambda_G
\end{pmatrix}  \cdot \begin{pmatrix}
Q_F & 0 \\
0 & Q_G
\end{pmatrix}  = Q^T \cdot \Lambda \cdot Q$$
where the $n$ eigenvectors of $A^TA,\ Q = \begin{pmatrix}
q_1, \ldots, q_m, q_{m+1}, \ldots q_n
\end{pmatrix}$ are the $n$ right singular vectors of $A$; where the first $m$ right singular vectors are the eigenvectors associated with the space of even functions, $F$, and the latter $m-n$ right singular vectors are the eigenvectors associated with the space of odd functions, $G$.
\\Now, for each $j \in \{1, \ldots, n\}$, we have --
$$\sigma_j u_j = Aq_j$$
whence we have $m$ even left singular vectors $\{u_j\}_{1 \le j \le m}$, and $n-m$ odd left singular vectors $\{u_j\}_{m+1 \le j \le n}$.
\end{proof}

\begin{question}[4]
\end{question}
\begin{proof}
$\mathbf{(a)}$ Suppose $G$ is isometric. Then for $v \in N_G^{\perp}$, $\|G v \| = \|v\|$. Additionally, $G$ preserves the inner product on $N_G^{\perp}$ -- Let $u,v \in N_G^{\perp}$. Then,
$$\|u\|^2 -2 \langle Gu, Gv \rangle + \|v\|^2 = \|Gu\|^2 -2 \langle Gu, Gv \rangle + \|Gv\|^2 = \|Gu - Gv \|^2$$
$$= \|G (u-v)\|^2 = \|u - v\|^2  = \|u\|^2 -2 \langle u, v \rangle + \|v\|^2$$
$$\iff \langle Gu, Gv \rangle = \langle u, v \rangle\ \ \ \ \ \ \ \ \ \ \ \ (11.1)$$
Then for any $u, v \in N_G^{\perp}$, we have --
$$\langle u, v \rangle = \langle Gu, Gv \rangle = \langle u, G^TG v \rangle \iff 0 = \langle u, G^TGv \rangle - \langle u, v \rangle  = \langle u, G^TGv - v \rangle $$
In particular, for $u = G^TG v - v$, we have --
$$\|G^TGv - v\|^2 = \langle G^TGv - v, G^TGv - v \rangle  = 0$$
$$\iff G^TGv - v = 0 \iff G^TGv = v$$
since $v \in N_G^{\perp}$ is arbitrary. That is, $G^TG$ acts as an identity on $N_G^{\perp}$.
\\Now, for $v \in N_G$, we have
$$G^TGv= G^T \cdot 0 = 0$$
Let $x \in X = N_G^{\perp} \oplus N_G$. Then $x$ can be written uniquely: $x = u +v$, for $u \in N_G^{\perp},\\ v \in N_G$. We have --
$$G^TGx = G^TG(u+v) = G^TGu + G^TGv = u + 0 = u$$
Obviously $N_G^{\perp} \perp N_G$ by definition, whence we have that $G^TG$ is the orthogonal projection of $X$ onto $N_G^{\perp}$.
\\$\mathbf{(b)}$ $X = N_G^{\perp} \oplus N_G$. So,
$$u \in N_G^{\perp} \iff u \not \in N_G \iff Gu \ne 0 \iff N_{G \restriction_{N_G^{\perp}}} = \{0\}$$
$\iff G \restriction_{N_G^{\perp}}$ is bijective and hence invertible. Additionally, for $0 \ne u \in N_G^{\perp}$, we showed in $\mathbf{(a)}$ that
$$G^TGu = u \ne 0 \iff 0 \ne Gu \not\in N_{G^T} \iff 0 \ne Gu \in N_{G^T}^{\perp}$$
(note that this also shows that $G \restriction_{N_G^{\perp}}$ is bijective). Since $\dim N_G^{\perp} = \dim N_{G^T}^{\perp}$ as the row rank, and column rank of $G$, respectively, it follows that $G \restriction_{N_G^{\perp}}: N_G^{\perp} \rightarrow N_{G^T}^{\perp}$ is a bijection. Thus, for any $v \in N_{G^T}^{\perp}$, there exists $u \in N_G^{\perp}$ such that $Gu = v$. Since $G$ is isometric, and $G^TG$ acts as an identity on $N_G^{\perp}$, we have --
$$\|G^Tv\| = \|G^TGu\| = \|u\| = \|Gu\| = \|v\|$$
\end{proof}

\begin{question}[5]
\end{question}
\begin{proof}
$\mathbf{(a)}$ See $(11.1)$.
\\$\mathbf{(b)}$ By question $(4)(a)$, it follows that $G^TG$ is an orthogonal projection. That is, taking all the singular values to be the positive square roots, we have $\sigma_i  = 1$. Since this gives a diagonal matrix $\Sigma$, it follows that $\Sigma$ is diagonalized by the identity matrix, hence the matrix of right singular vectors is $V = I$. I.e., $v_i = e_i$; whence this gives
$$u_i = \sigma_i \cdot u_i = G v_i = G e_i = g_i$$
for $1 \le i \le k$. As indeed is expected since the first $k$ columns of $G$ must be orthogonal if $G^TG$ is to be an orthogonal projection. Thus $G = \sum_{i=1}^k \sigma_i u_i v_i^T$ is the desired reduced SVD.
\\$\mathbf{(c)}$
\end{proof}

\end{document}
