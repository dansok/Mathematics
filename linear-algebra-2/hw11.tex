\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm, mathrsfs}
\addtolength{\evensidemargin}{-.5in}
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\textwidth}{0.8in}
\addtolength{\textheight}{0.8in}
\addtolength{\topmargin}{-.4in}
\newtheoremstyle{quest}{\topsep}{\topsep}{}{}{\bfseries}{}{ }{\thmname{#1}\thmnote{ #3}.}
\theoremstyle{quest}
\newtheorem*{definition}{Definition}
\newtheorem*{theorem}{Theorem}
\newtheorem*{question}{Question}
\newtheorem*{exercise}{Exercise}
\newtheorem*{challengeproblem}{Challenge Problem}
\newcommand{\name}{%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% put your name here, so we know who to give credit to %%
Dan Sokolsky
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\hw}{%%%%%%%%%%%%%%%%%%%%
%% and which homework assignment is it? %%%%%%%%%
%% put the correct number below              %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
11
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\vspace{-50pt}
\Huge \name
\\\vspace{20pt}
\huge Linear Algebra II\hfill Homework \hw}
\author{}
\date{}
\pagestyle{myheadings}
\markright{\name\hfill Homework \hw\qquad\hfill}

%% If you want to define a new command, you can do it like this:
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

%% If you want to use a function like ''sin'' or ''cos'', you can do it like this
%% (we probably won't have much use for this)
% \DeclareMathOperator{\sin}{sin}   %% just an example (it's already defined)


\begin{document}
\maketitle

\begin{question}[1]
\end{question}
\begin{proof}
$\mathbf{(a)}$ Gaussian Elimination is a set of row operations.
\\$\mathbf{(b)}$ Let $A = \begin{pmatrix}
a_{11} & * \\
\vec{a} & * \end{pmatrix}$ where $\vec{a} = \begin{pmatrix}
a_{21} \\
\vdots \\
a_{n1}
\end{pmatrix}$. Let $G = \begin{pmatrix}
1 & 0 \\
\vec{g} & I_{n-1}
\end{pmatrix}$ where $\vec{g} = \begin{pmatrix}
-\frac{a_{21}}{a_{11}} \\
\vdots \\
-\frac{a_{n1}}{a_{11}}
\end{pmatrix}$
Then $GA = \begin{pmatrix}
a_{11} & * \\
0 & *
\end{pmatrix} $ as required.
\\$\mathbf{(c)}$ $G = I_n - g \cdot e_1^T$ where similarly to what is defined above --
$$g = \begin{pmatrix}
\frac{a_{21}}{a_{11}} \\
\vdots \\
\frac{a_{n1}}{a_{11}}
\end{pmatrix},\ e_1 = \begin{pmatrix}
1 \\
0 \\
\vdots \\
0
\end{pmatrix}$$
and where $g \cdot e_1^T$ is the desired rank$-1$ matrix in outer product form.
\end{proof}

\begin{question}[2]
\end{question}
\begin{proof}
$\mathbf{(a)}$ $A = I_n + u \cdot e_1^T$ where
$$u = \begin{pmatrix}
0 \\
u_1 \\
\vdots \\
u_{n-1}
\end{pmatrix},\ e_1 = \begin{pmatrix}
1 \\
0 \\
\vdots \\
0
\end{pmatrix}$$
\\$\mathbf{(b)}$ $A$ clearly has full rank as a lower triangular matrix. Thus $A$ is invertible. Denote $X = A^{-1}$ for convenience, and observe --
$$I_n = XA = X(I + B) = X + XB = \begin{pmatrix}
x_{11} & \ldots & x_{1n} \\
\vdots & \ddots & \vdots \\
x_{n1} & \ldots & x_{nn}
\end{pmatrix} + \begin{pmatrix}
x_{11} & \ldots & x_{1n} \\
\vdots & \ddots & \vdots \\
x_{n1} & \ldots & x_{nn}
\end{pmatrix} \cdot \begin{pmatrix}
0 & \ldots & 0 \\
u_1 & \ldots & 0 \\
\vdots & \ddots & \vdots \\
u_{n-1} & \ldots & 0
\end{pmatrix}$$
$$= \begin{pmatrix}
x_{11} & \ldots & x_{1n} \\
\vdots & \ddots & \vdots \\
x_{n1} & \ldots & x_{nn}
\end{pmatrix} + \begin{pmatrix}
x_{12} u_1 + \ldots + x_{1n} u_{n-1} & 0 & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
x_{n2} u_1 + \ldots + x_{nn} u_{n-1} & 0 & \ldots & 0
\end{pmatrix}$$
From which it immediately follows that $x_{jj} = 1$ for $j \ge 2$, and $x_{ij} = 0$ for $i \ne j,\\ j \ge 2$. It follows that $(XB)_{11} = 0$ and therefore $x_{11} = 1$. Finally, we see that
$$XB = \begin{pmatrix}
0 & 0 & \ldots & 0 \\
u_1 & 0 & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
u_{n-1} & 0 & \ldots & 0
\end{pmatrix} $$
Therefore, we must have --
$$A^{-1} = X = \begin{pmatrix}
1 & 0 & \ldots & 0 \\
-u_1 & 1 & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
-u_{n-1} & 0 & \ldots & 1
\end{pmatrix} = I_n -u \cdot e_1^T$$
\\$\mathbf{(c)}$ Supppose $A$ is invertible, i.e. $A^{-1}$ exists and is unique. Then, given $x$,
$$x + uv^Tx = (I + uv^T)x = Ax = y$$
$$\iff x = y - uv^Tx$$
$$\iff v^Tx = v^Ty - v^Tuv^Tx$$
$$\iff (1 + v^Tu)v^Tx = v^Ty$$
$$\iff v^Tx = \dfrac{v^Ty}{1 + v^Tu}$$
$$\iff x = y - uv^Tx = y - u \cdot \dfrac{v^Ty}{1 + v^Tu} = \Big(I - \dfrac{uv^T}{1 + v^Tu}\Big) \cdot y$$
That is, $\Big(I - \dfrac{uv^T}{1 + v^Tu}\Big)$ maps $y$ back to $x$. Since $x$ is arbitrary, and $A^{-1}$ is unique, we have $A^{-1} = I - \dfrac{uv^T}{1 + v^Tu}$.
\end{proof}

\begin{question}[3]
\end{question}
\begin{proof}
WLOG, suppose $A = USV^T, S\ k \times k$ is a tall matrix in reduced SVD form. Let $S$ be $k \times k$. $U, V$ are orthogonal. By definition $2$ of homework $9$, we need to show that $U, V^T$ are isometric. We will show that orthogonal matrices are isometric. Question $(4)(b)$ tells us that $V^T$ is isometric as well.
We show $U$ is isometric, and the argument for $V$ is identical. Note that $U^TU = I_k$, and we have for all $x$ --
$$\|Ux\|^2 = \langle Ux, Ux \rangle = \langle x, U^TUx \rangle = \langle x, Ix \rangle =\langle x, x \rangle = \|x\|^2$$
$\iff \|Ux\| = \|x\|$. That is, $U$ is isometric. Likewise, $V$ is isometric. By question $(4)(b)$, $V^T$ is isometric. Finally, a quick note on the inclusion of the row and column spaces as mentioned in definition $2$ --
$$0 \ne Sx \in Range(S) \implies \|USx\| = \|Sx\| > 0 \iff USx \ne 0$$
Thus $Sx \not\in N_U \iff Sx \in N_U^{\perp}$. We have $columnspace(S) \subseteq rowspace(U)$.
\\$rowspace(S) \subseteq columnspace(V^T)$ is trivial. This shows that $U, V$ were properly selected (but of course this is obvious by construction of the SVD).
\end{proof}

\begin{question}[4]
\end{question}
\begin{proof}
$\mathbf{(a)}$ Suppose $G$ is isometric. Then for $v \in N_G^{\perp}$, $\|G v \| = \|v\|$. Additionally, $G$ preserves the inner product on $N_G^{\perp}$ -- Let $u,v \in N_G^{\perp}$. Then,
$$\|u\|^2 -2 \langle Gu, Gv \rangle + \|v\|^2 = \|Gu\|^2 -2 \langle Gu, Gv \rangle + \|Gv\|^2 = \|Gu - Gv \|^2$$
$$= \|G (u-v)\|^2 = \|u - v\|^2  = \|u\|^2 -2 \langle u, v \rangle + \|v\|^2$$
$$\iff \langle Gu, Gv \rangle = \langle u, v \rangle\ \ \ \ \ \ \ \ \ \ \ \ (11.1)$$
Then for any $u, v \in N_G^{\perp}$, we have --
$$\langle u, v \rangle = \langle Gu, Gv \rangle = \langle u, G^TG v \rangle \iff 0 = \langle u, G^TGv \rangle - \langle u, v \rangle  = \langle u, G^TGv - v \rangle $$
In particular, for $u = G^TG v - v$, we have --
$$\|G^TGv - v\|^2 = \langle G^TGv - v, G^TGv - v \rangle  = 0$$
$$\iff G^TGv - v = 0 \iff G^TGv = v$$
since $v \in N_G^{\perp}$ is arbitrary. That is, $G^TG$ acts as an identity on $N_G^{\perp}$.
\\Now, for $v \in N_G$, we have
$$G^TGv= G^T \cdot 0 = 0$$
Let $x \in X = N_G^{\perp} \oplus N_G$. Then $x$ can be written uniquely: $x = u +v$, for $u \in N_G^{\perp},\\ v \in N_G$. We have --
$$G^TGx = G^TG(u+v) = G^TGu + G^TGv = u + 0 = u$$
Obviously $N_G^{\perp} \perp N_G$ by definition, whence we have that $G^TG$ is the orthogonal projection of $X$ onto $N_G^{\perp}$.
\\$\mathbf{(b)}$ $X = N_G^{\perp} \oplus N_G$. So,
$$u \in N_G^{\perp} \iff u \not \in N_G \iff Gu \ne 0 \iff N_{G \restriction_{N_G^{\perp}}} = \{0\}$$
$\iff G \restriction_{N_G^{\perp}}$ is bijective and hence invertible. Additionally, for $0 \ne u \in N_G^{\perp}$, we showed in $\mathbf{(a)}$ that
$$G^TGu = u \ne 0 \iff 0 \ne Gu \not\in N_{G^T} \iff 0 \ne Gu \in N_{G^T}^{\perp}$$
(note that this also shows that $G \restriction_{N_G^{\perp}}$ is bijective). Since $\dim N_G^{\perp} = \dim N_{G^T}^{\perp}$ as the row rank, and column rank of $G$, respectively, it follows that $G \restriction_{N_G^{\perp}}: N_G^{\perp} \rightarrow N_{G^T}^{\perp}$ is a bijection. Thus, for any $v \in N_{G^T}^{\perp}$, there exists $u \in N_G^{\perp}$ such that $Gu = v$. Since $G$ is isometric, and $G^TG$ acts as an identity on $N_G^{\perp}$, we have --
$$\|G^Tv\| = \|G^TGu\| = \|u\| = \|Gu\| = \|v\|$$
\end{proof}

\begin{question}[5]
\end{question}
\begin{proof}
Suppose $\dim V = n, \dim U = m$.
\\$\mathbf{(a)}$ See $(11.1)$.
\\$\mathbf{(b)}$ By question $(4)(a)$, it follows that $G^TG$ is an orthogonal projection (WLOG suppose the first k entries on the diagonal are ones. That is, suppose the first $k$ column vectors of $G$ are the orthonormal ones). That is, taking all the singular values to be the positive square roots, we have $\sigma_i  = 1$. Since this gives a diagonal matrix $\Sigma$, it follows that $\Sigma$ is diagonalized by the identity matrix, hence the matrix of right singular vectors is $V = I_{k \times n}$. I.e., $v_i = e_i$; whence this gives
$$u_i = \sigma_i \cdot u_i = G v_i = G e_i = g_i$$
for $1 \le i \le k$. As indeed is expected since the first $k$ columns of $G$ must be orthonormal if $G^TG$ is to be an orthogonal projection. Thus $G = \sum_{i=1}^k \sigma_i u_i v_i^T = \sum_{i=1}^k g_i e_i^T$ is the desired reduced SVD. In matrix form --
$$G = \begin{pmatrix}
g_1 & \ldots & g_k
\end{pmatrix} \cdot I_k \cdot I_{k \times n}$$
\\$\mathbf{(c)}$ Since $\{g_1, \ldots, g_k\}$ spans $N_G^{\perp}$ (and is orthonormal), it follows that $\{g_{k+1}, \ldots, g_m\}$ spans $N_G$. Let us orthogonalize the latter set using Gram-Schmidt to give an orthonormal basis to $N_G$. For $k+1 \le j \le m$,
$$\hat{g}_j = g_j - \sum_{i=1}^{j-1}\dfrac{\langle g_i, g_j \rangle}{\|g_i\|^2}g_i$$
That is, the full SVD in matrix form is --
$$G = \begin{pmatrix}
g_1 & \ldots & g_k & \hat{g}_{k+1} & \ldots & \hat{g}_m
\end{pmatrix} \cdot I_{m \times n} \cdot I_n = U \Sigma V^T$$
where $\Sigma$ is diagonal, and $U, V$ are both orthogonal.
\end{proof}

\end{document}
