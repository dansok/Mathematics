\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\addtolength{\evensidemargin}{-.5in}
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\textwidth}{0.8in}
\addtolength{\textheight}{0.8in}
\addtolength{\topmargin}{-.4in}
\newtheoremstyle{quest}{\topsep}{\topsep}{}{}{\bfseries}{}{ }{\thmname{#1}\thmnote{ #3}.}
\theoremstyle{quest}
\newtheorem*{definition}{Definition}
\newtheorem*{theorem}{Theorem}
\newtheorem*{question}{Question}
\newtheorem*{exercise}{Exercise}
\newtheorem*{challengeproblem}{Challenge Problem}
\newcommand{\name}{%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% put your name here, so we know who to give credit to %%
Dan Sokolsky
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\hw}{%%%%%%%%%%%%%%%%%%%%
%% and which homework assignment is it? %%%%%%%%%
%% put the correct number below              %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
5
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\vspace{-50pt}
\Huge \name
\\\vspace{20pt}
\huge Linear Algebra II\hfill Homework \hw}
\author{}
\date{}
\pagestyle{myheadings}
\markright{\name\hfill Homework \hw\qquad\hfill}

%% If you want to define a new command, you can do it like this:
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

%% If you want to use a function like ''sin'' or ''cos'', you can do it like this
%% (we probably won't have much use for this)
% \DeclareMathOperator{\sin}{sin}   %% just an example (it's already defined)


\begin{document}
\maketitle

\begin{question}[1]
Let $X$ be a linear space such that $\oplus_j V_j = X = \oplus_j U_j$. Prove $U_j \subset V_j \implies U_j = V_j$.
\end{question}
\begin{proof}
$V_i \cap V_j = \{0\}$ for $i \ne j$. In particular, $U_j \subset V_j \implies U_j \cap V_i = \{0\}$ for $i \ne j$. Thus $x \in V_j \setminus U_j \implies x \notin U_i$ for any $i$. Thus $x \notin \oplus_j U_j = X = \oplus_j V_j$. That is, $x \notin V_i$ for any $i$, a contradiction. Therefore, $V_j \setminus U_j = \emptyset$. Thus, $U_j = V_j$. 
\end{proof}
\begin{question}[2]
Let $A$ be $n \times n$ of rank 2. Classify the minimal polynomial of $A$.
\end{question}
\begin{proof}
$n \ge 2$ since $rank(A) = 2$.
\\$\underline{If\ n = 2}$, then either $A$ is diagonalizable, or it is similar to a $2 \times 2$ Jordan block. If it is diagonalizable, then -- if the eigenvalues are distinct, $m_A(t) = (t - \lambda_1)(t - \lambda_2)$. If the eigenvalue is repeated, $m_A(t) = t - \lambda_1$ (there are two $1 \times 1$ Jordan blocks for the eigenvalue $\lambda_1$). Else, If it is similar to a Jordan block, $m_A(t) = (t - \lambda_1)^2$. In either case, $\lambda_1, \lambda_2 \ne 0$ since if even a single eigenvalue is $0$, say $\lambda_1 = 0$, then it follows that $rank(A) \le 1$, contradicting $rank(A)=2$.
\\$\underline{If\ n \ge 3}$, then $\Big A \sim \begin{pmatrix} X & 0 \\ 0 & 0 \end{pmatrix}$ (the Jordan Normal Form) under the basis $\{v_1, v_2, w_1,\ldots,w_{n-2}\}$; where $\{v_1, v_2\}$ are basis elements for the (generalized) eigenspace(s) that make up $X$ and $\{w_i\}_{i \le n-2}$ is a basis for $N_A$. Thus $X$ is $2 \times 2$, $rank(X)=2$, and the nature of it's minimal polynomial has been discussed in the $n=2$ case. The lower right block matrix is a diagonal matrix with all zeros on the diagonal ($1 \times 1$ Jordan blocks). Thus the minimal polynomial may have one of the following forms -- $m_A(t) = t(t - \lambda_1)(t - \lambda_2)$; $m_A(t) = t(t - \lambda_1)$; or $m_A(t) = t(t - \lambda_1)^2$.
\end{proof}
\begin{question}[3]
Let $A \sim J$, $J$ an $n \times n$ super-diagonal Jordan block with all zeros on the diagonal. Suppose $A = SJS^{-1}$ for some invertible matrix $S$. Define the $n \times n$ matrix $\mathcal{E}$ by $\mathcal{E}_{n, 1} = 10^{-n}$ and $\mathcal{E}_{i, j} = 0$ for $(i, j) \ne (n, 1)$. Define $\tilde{J} := J + \mathcal{E}$. Let $\tilde{A} = S\tilde{J}S^{-1}$. Let $H$ be the Jordan Normal Form of $\tilde{J}$. Can we expect $H$ to be close to $J$? Find the eigenvalues of $\tilde{J}$ and determine what $H$ looks like.
\end{question}
\begin{proof}
For one thing, $J$ has $n-1$ linearly independent columns, whereas $\tilde{J}$ has $n$ linearly independent columns. That is, $\tilde{J}$ has full rank, whereas $J$ doesn't. We will see that $\tilde{J}$ is diagonalizable, i.e., $H$ is diagonal; and the eigenvalues on the diagonal are $\frac{1}{10}$ in modulus, whence $H$ and and $J$ are not close. To that end, let us find the eigenvalues --
$$\begin{pmatrix}
v_2 \\ v_3 \\ \vdots \\ v_n \\ 10^{-n}v_1
\end{pmatrix}= \tilde{J} \begin{pmatrix}
v_1 \\ \vdots \\ v_n
\end{pmatrix} = \tilde{J}v = \lambda v = \lambda \begin{pmatrix}
v_1 \\ \vdots \\ v_n
\end{pmatrix} = \begin{pmatrix}
\lambda v_1 \\ \vdots \\ \lambda v_n
\end{pmatrix}$$
$$\iff v_2 = \lambda v_1;\ v_3 = \lambda v_2;\ \ldots ;\ 10^{-n} v_1 = \lambda v_n$$
$$\iff 10^{-n} v_1 = \lambda^2 v_{n-2} = \lambda^3 v_{n-3} = \ldots = \lambda^n v_1$$
$$\iff 10^{-n} = \lambda^n \iff (10\lambda)^n = 1$$
$\iff 10\lambda$ is an $n^{th}$ root of unity $\iff$ the $n$ solutions are the following -- 
\\$\{\lambda_k = \frac{1}{10}\exp(\frac{2k\pi i}{n}) = \frac{1}{10}(\cos \frac{2k\pi}{n} + i\cdot\sin \frac{2k\pi}{n})\}_{1 \le k \le n}.$ In particular, $$|\lambda_k| = \frac{1}{10} \Big|\cos\frac{2k\pi}{n} + i \cdot \sin \frac{2k\pi}{n} \Big| = \frac{1}{10} \cdot 1 = \frac{1}{10}$$
and notice that the $\{\lambda_k\}$ are distinct. That is, the eigenspaces $\{N_{\tilde{J}-\lambda_k I}\}$ are distinct. It follows that $\tilde{J}$ is diagonalizable. That is, $H_{k,k} = \lambda_k$ and $H_{i, j} = 0$ for $i \ne j$.
\end{proof}
\begin{question}[4]
Let $A: \mathbb{C}^n \rightarrow \mathbb{C}^n$. Determine necessary and sufficient conditions on $A^k$ such that $R_k \cap N_k = \{0\}$, whence $R_k \oplus N_k = \mathbb{C}^n$.
\end{question}
\begin{proof}
Define $R_i := Range(A^i),\ N_i := Null(A^i)$, as usual, keeping to the convention $A^0 = I$. We claim there exists a minimal $k$ such that $R_k = R_{k+1}$. That also implies $N_k = N_{k+1}$, and consequently $R_k \cap N_k = \{0\}$, from which a simple dimension argument gives $R_k \oplus N_k = \mathbb{C}^n$. To this end, first, we claim --
$$\{0\} = N_0 \subseteq N_1 \subseteq N_2 \subseteq \ldots \subseteq N_i \subseteq \ldots \subseteq \mathbb{C}^n\ \ \ \ \ (1)$$
$$\mathbb{C}^n = R_0 \supseteq R_1 \supseteq R_2 \supseteq \ldots \supseteq R_j \supseteq \ldots \supseteq \{0\}\ \ \ \ \ (2)$$
$(1)$ we proved in homework $#1$. To see $(2)$, observe \\$y \in R_{j+1} \implies y = A^{j+1}x = A^j(Ax)$ for some $x$. That is, $y \in R_j$.
\\Now, from $(1)$ and $(2)$,
 $$0 = \dim N_0 \le \dim N_1 \le \dim N_2 \le \ldots \le \dim N_i \le \ldots \le n\ \ \ \ \ (3)$$
$$n = \dim R_0 \ge \dim R_1 \ge \dim R_2 \ge \ldots \ge \dim R_j \ge \ldots \ge 0\ \ \ \ \ (4)$$
We claim there must exist $k$ such that $R_k = R_{k+1}$, for otherwise the inclusion in $(2)$ is always proper, and thus the dimensions of the successive $R_j$ in $(4)$ decrease indefinitely, eventually going below $0$, which is impossible since they are bounded by $0$ from below. Now a dimension argument shows $N_k = N_{k+1}$ -- Let $m = \dim N_k$. Then $\dim R_{k+1} = \dim R_k = n-m$, by Rank-Nullity. Thus $\dim N_{k+1} = n-(n-m) = m$. Since $N_k \subseteq N_{k+1}$ and the two linear spaces have the same dimension, we conclude $N_k = N_{k+1}$. As before, by induction $N_k = N_{k+l}$ for any $l > 0$. \\Finally, let $y \in R_k \cap N_k$. Then $y = A^k x$ for some $x$. Also, $A^k y = 0$. Putting these two together, $0 = A^k y = A^k(A^k x ) = A^{2k} x \iff x \in N_{2k} = N_k \iff y = A^k x = 0$. Thus $R_k \cap N_k = \{0\}$. \\We show $R_k \oplus N_k  = \mathbb{C}^n$ -- Let $X = \{v_i\}_{i\le p},\ Y = \{w_j\}_{j \le q}$ be bases for $R_k, N_k$, respectively. Then $\dim R_k + \dim N_k = p + q = n$. Suffices to prove $X \cup Y$ is linearly independent. To that end,
suppose $\sum_{i \le p} c_i v_i + \sum_{j \le q} k_j w_j = 0$ and for the sake of contradiction suppose some of the coefficients $\{c_1, \ldots, c_p, k_1, \ldots, k_q\}$ are nonzero. Suppose WLOG $c_i \ne 0$. Then $0 \ne u = \sum_{i \le p} c_i v_i = -\sum_{j \le q} k_j w_j$. So $0 \ne u \in R_k \cap N_k$, a contradiction. Therefore $c_i = k_j = 0$, and $X \cup Y$ forms a basis for $\mathbb{C}^n$. Thus $R_k \oplus N_k = \mathbb{C}^n$.
\end{proof}
\begin{question}[5]
Let $G = \begin{pmatrix}
B & 0 \\
0 & C
\end{pmatrix}$. Let $p(t)$ be a polynomial
\\(a) Express $p(G)$ in terms of $B$, $C$.
\\(b) Use Jordan Normal Form to deduce the Cayley-Hamilton theorem.
\end{question}
\begin{proof}
(a) Observe that for a block diagonal matrix, $G^2 = \begin{pmatrix}
B & 0 \\
0 & C
\end{pmatrix}^2 = \begin{pmatrix}
B^2 & 0 \\
0 & C^2
\end{pmatrix}$.\\By induction, $G^k = \begin{pmatrix}
B & 0 \\
0 & C
\end{pmatrix}^k = \begin{pmatrix}
B^k & 0 \\
0 & C^k
\end{pmatrix}$. Thus,
$$p(G) = c_0I + c_1 G + \ldots + c^k G^k + \ldots + c_n G^n = $$
$$c_0I + c_1\begin{pmatrix}
B & 0 \\
0 & C
\end{pmatrix} + \ldots + c_k \begin{pmatrix}
B^k & 0 \\
0 & C^k
\end{pmatrix} + \ldots + c_n \begin{pmatrix}
B^n & 0 \\
0 & C^n
\end{pmatrix} =$$
$$\begin{pmatrix}
c_0I + c_1 B + \ldots + c_k B^k + \ldots + c_n B^n & 0 \\
0 & c_0I + c_1 C + \ldots + c_k C^k + \ldots + c_n C^n 
\end{pmatrix} =$$
$$\begin{pmatrix}
p(B) & 0 \\
0 & p(C)
\end{pmatrix}$$
\\(b) Let A be a matrix and let $p_A(t)$ be its characteristic polynomial. A is similar to it's Jordan Normal Form, $A \sim J$, where $J = \begin{pmatrix}
J_1 & 0 & \ldots & 0\\
0 & J_2 & \ldots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \ldots & J_k
\end{pmatrix}$, $J_i$ are Jordan blocks. That is, $A = SJS^{-1}$ for some invertible $S$. By (a), suffices to prove $p_A(J_i) = 0$. Recall $p_A(t) = \prod_{j \le k} (t - \lambda_j)^{m_j}$, where $\lambda_j$ is the eigenvalue of the the Jordan block $J_j$. So $p_A(J_i) = \prod_{j \le k} (J_i - \lambda_j I)^{m_j} =0 $, because $(J_i - \lambda_i I)^{m_i} = 0$, since $m_i \ge d_i$, the index of the eigenvalue $\lambda_i$. This proves $p_A(J) = 0$. Now, $$(SJS^{-1})^n = (SJS^{-1}) \cdot (SJS^{-1}) \cdot \ldots \cdot (SJS^{-1}) = SJ^nS^{-1}$$ \\ \\So,
$$p_A(A) = p_A(SJS^{-1}) = c_0I + c_1 SJS^{-1} +c_2 (SJS^{-1})^2 + \ldots + c_n (SJS^{-1})^n =$$
$$c_0I + c_1 SJS^{-1} +c_2 (SJS^{-1})^2 + \ldots + c_n (SJS^{-1})^n =$$
$$c_0I + c_1 SJS^{-1} +c_2 SJ^2S^{-1} + \ldots + c_n SJ^nS^{-1} =$$
$$S(c_0I + c_1 J +c_2 J^2 + \ldots + c_n J^n)S^{-1} =$$
$$S \cdot p_A(J) \cdot S^{-1} = S \cdot 0 \cdot S^{-1} = 0$$
\end{proof}

\end{document}
