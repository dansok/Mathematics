\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm, mathrsfs}
\addtolength{\evensidemargin}{-.5in}
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\textwidth}{0.8in}
\addtolength{\textheight}{0.8in}
\addtolength{\topmargin}{-.4in}
\newtheoremstyle{quest}{\topsep}{\topsep}{}{}{\bfseries}{}{ }{\thmname{#1}\thmnote{ #3}.}
\theoremstyle{quest}
\newtheorem*{definition}{Definition}
\newtheorem*{theorem}{Theorem}
\newtheorem*{question}{Question}
\newtheorem*{exercise}{Exercise}
\newtheorem*{challengeproblem}{Challenge Problem}
\newcommand{\name}{%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% put your name here, so we know who to give credit to %%
Dan Sokolsky
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\hw}{%%%%%%%%%%%%%%%%%%%%
%% and which homework assignment is it? %%%%%%%%%
%% put the correct number below              %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
10
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\vspace{-50pt}
\Huge \name
\\\vspace{20pt}
\huge Linear Algebra II\hfill Homework \hw}
\author{}
\date{}
\pagestyle{myheadings}
\markright{\name\hfill Homework \hw\qquad\hfill}

%% If you want to define a new command, you can do it like this:
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

%% If you want to use a function like ''sin'' or ''cos'', you can do it like this
%% (we probably won't have much use for this)
% \DeclareMathOperator{\sin}{sin}   %% just an example (it's already defined)


\begin{document}
\maketitle

\begin{question}[1]
\end{question}
\begin{proof}
$\mathbf{(a)}$ Since $A \ne 0$, there exists $w \in \mathbb{R}^{n}$ such that $Aw \ne 0$. Thus
\\$\|Av\| = \|A\| \ge \|Aw\| > 0$; whence $Av \ne 0$. That is $v \not\in N_A \iff v \in N_A^{\perp}$ since $N_A \oplus N_A^{\perp} = \mathbb{R}^n$.
\\$\mathbf{(b)}$ $\|v\| = 1$, so
$$\sigma = \|A\| = \|Av\| = \langle Av, \frac{Av}{\|Av\|} \rangle = \langle Av, \frac{\sigma u}{\sigma} \rangle = \langle Av, u \rangle = \langle v, A^T u \rangle$$
Now,
$$\sup_{\|x\|=1, \|w\| = 1}\langle A^Tx, w \rangle = \|A^T\| = \|A\| = \sigma = \langle A^Tu, v  \rangle = \sup_{\|w\|=1} \langle A^Tu, w \rangle = \|A^Tu\|$$
That is, $A^T$ achieves it's maximum for $x = u,\ w=v$. That is,
$$\langle A^Tu, v  \rangle = \sigma = \|A^T\| = \|A^Tu\| = \|A^Tu\| \cdot 1 = \|A^Tu\| \cdot \|v\|$$
$$\iff A^Tu = k \cdot v \iff v = \dfrac{A^Tu}{k}$$
since by Cauchy-Schwartz, equality is achieved $\iff$ the two vectors are scalar multiples of one another. Now,
$$\|A^Tu\| = \langle A^Tu, v\rangle = \langle A^Tu, \dfrac{A^Tu}{k}\rangle = \dfrac{1}{k} \langle A^Tu, A^Tu \rangle = \dfrac{1}{k} \|A^Tu\|^2$$
$$\iff k = \|A^Tu\| = \sigma$$
That is, $A^Tu = \sigma v$.

\end{proof}

\begin{question}[2]
\end{question}
\begin{proof}
$\mathbf{(a)}$ We iterate step 1. Find a unit vector $v_2 \in \mathbb{R}^n$ such that $\|A_2\| = \|A_2v_2\|$.
\\Let $\sigma_2 u_2 = A_2 v_2,\ \sigma_2 = \|A_2\|$ so that $u_2 \in \mathbb{R}^m$ is a unit vector.
\\$\mathbf{(b)}$ By question $(1)(a)$, $v_2 \in N_{A_2}^{\perp}$. Define $A_3 = A_2 - \sigma_2 u_2 v_2^T$. Then
$$A_3 v_2 = A_2 v_2 - \sigma_2 u_2 v_2^Tv_2 = A_2 v_2 - \sigma_2 u_2 \|v_2\|^2 = A_2 v_2 - \sigma_2 u_2 = 0$$
That is, $v_2 \in N_{A_3}$.
\\$\mathbf{(c)}$ Since $v_1 \in N_{A_2}$ and $v_2 \in N_{A_2}^{\perp}$, it follows that $v_1 \perp v_2$, or $\langle v_1, v_2 \rangle = 0$.
\\$\mathbf{(d)}$ We have $A_i = A_{i-1} - \sigma_{i-1} u_{i-1} v_{i-1}^T = A - \Sigma_{j=1}^{i-1} \sigma_j u_j v_j^T$.
\end{proof}

\begin{question}[3]
\end{question}
\begin{proof}
$\mathbf{(a)}$ Let $A_1 = U_1 \Sigma_1 V_1^T,\ A_2 = U_2 \Sigma_2 V_2^T$. Define $U = \begin{pmatrix}
U_1 & 0 \\
0 & U_2
\end{pmatrix}$, $V = \begin{pmatrix}
V_1 & 0 \\
0 & V_2
\end{pmatrix}$, $\Sigma = \begin{pmatrix}
\Sigma_1 & 0 \\
0 & \Sigma_2
\end{pmatrix}$. Then $U, V$ are orthogonal matrices since $U_i, V_i$ are orthogonal matrices, and $\Sigma$ is diagonal since $\Sigma_1, \Sigma_2$ are diagonal. Then
\\$A = \begin{pmatrix}
A_1 & 0 \\
0 & A_2
\end{pmatrix} = \begin{pmatrix}
U_1 \Sigma_1 V_1^T & 0 \\
0 & U_2 \Sigma_2 V_2^T 
\end{pmatrix} = U \Sigma V^T$.
\\$\mathbf{(b)}$ Let $\beta = \{x_1, \ldots, x_m, x_{m+1}, \ldots, x_n\}$ be any basis for $X$ such that $\{x_1, \ldots, x_m\}$ spans $X_1$, and $\{x_{m+1}, \ldots, x_n\}$ spans $X_2$. Since $AX_1 \subseteq X_1$ and $AX_2 \subseteq X_2$, $A \restriction_{X_1} : X_1 \rightarrow X_1$ and $A \restriction_{X_2} : X_2 \rightarrow X_2$, and we can write w.r.t $\beta$ --
$$A =  \begin{pmatrix}
A \restriction_{X_1} & 0 \\
0 & A \restriction_{X_2}
\end{pmatrix} = \begin{pmatrix}
U_1 \Sigma_1 V_1^T & 0 \\
0 & U_2 \Sigma_2 V_2^T
\end{pmatrix} = U \cdot \Sigma \cdot V^T$$
as in $\mathbf{(a)}$.
\\$\mathbf{(c)}$ Let $A = \begin{pmatrix}
f_1 & \ldots & f_m & f_{m+1} & \ldots & f_n
\end{pmatrix}$. Let $F = span \{f_1, \ldots, f_m\}$, $G = span \{f_{m+1}, \ldots, f_n\}$, $F \oplus G = X$. For $f \in F, g \in G$, $f \cdot g$ is an odd function, so $\langle f, g \rangle = \int_{-1}^{-1} f \cdot g\ dx = 0$. Thus $F \perp G$. Thus
$$A^TA = \begin{pmatrix}
f_1 \\ \vdots \\ f_m \\ f_{m+1} \\ \vdots \\ f_n
\end{pmatrix} \cdot \begin{pmatrix}
f_1 & \ldots & f_m & f_{m+1} & \ldots & f_n
\end{pmatrix} =$$
$$\begin{pmatrix}
\langle f_1, f_1 \rangle & \ldots & \langle f_1, f_m \rangle & 0 & \ldots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
\langle f_m, f_1 \rangle & \ldots & \langle f_m, f_m \rangle & 0 & \ldots & 0 \\
0 & \ldots & 0 & \langle f_{m+1}, f_{m+1} \rangle & \ldots & \langle f_{m+1}, f_n \rangle 
 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \ldots & 0 & \langle f_n, f_{m+1} \rangle & \ldots & \langle f_n, f_n \rangle 
\end{pmatrix} = \begin{pmatrix}
B_F & 0 \\
0 & B_G
\end{pmatrix} $$
Thus $B_F, B_G$ and $A^TA$ are symmetric, and are therefore diagonalizable by orthogonal eigenvectors. I.e.,
$$A^TA = \begin{pmatrix}
B_F & 0 \\
0 & B_G
\end{pmatrix} = \begin{pmatrix}
Q^T_F \Lambda_F Q_F & 0 \\
0 & Q^T_G \Lambda_G Q_G
\end{pmatrix} =$$
$$\begin{pmatrix}
Q^T_F & 0 \\
0 & Q^T_G
\end{pmatrix}  \cdot \begin{pmatrix}
\Lambda_F & 0 \\
0 & \Lambda_G
\end{pmatrix}  \cdot \begin{pmatrix}
Q_F & 0 \\
0 & Q_G
\end{pmatrix}  = Q^T \cdot \Lambda \cdot Q$$
where the $n$ eigenvectors of $A^TA,\ Q = \begin{pmatrix}
q_1, \ldots, q_m, q_{m+1}, \ldots q_n
\end{pmatrix}$ are the $n$ right singular vectors of $A$; where the first $m$ right singular vectors are the eigenvectors associated with the space of even functions, $F$, and the latter $m-n$ right singular vectors are the eigenvectors associated with the space of odd functions, $G$.
\\Now, for each $j \in \{1, \ldots, n\}$, we have --
$$\sigma_j u_j = Aq_j$$
whence we have $m$ even left singular vectors $\{u_j\}_{1 \le j \le m}$, and $n-m$ odd left singular vectors $\{u_j\}_{m+1 \le j \le n}$.
\end{proof}

\begin{question}[4]
\end{question}
\begin{proof}
$\mathbf{(a)}$ Since $B, C$ are real symmetric positive definite, $B^{\frac{1}{2}}, C^{\frac{1}{2}}$ exist and are real symmetric positive definite themselves. Define $\tilde{A} = B^{\frac{1}{2}} A C^{\frac{1}{2}}$. Then $\tilde{A} = \tilde{U}  \tilde{\Sigma} \tilde{V}^T$. Let $U = B^{-\frac{1}{2}} \tilde{U},\ V = C^{-\frac{1}{2}} \tilde{V}$. Then $U, V$ are orthogonal, and $\tilde{\Sigma}$ is diagonal. Thus,
$$A = B^{-\frac{1}{2}} \tilde{A} C^{-\frac{1}{2}} = B^{-\frac{1}{2}} \tilde{U}  \tilde{\Sigma} \tilde{V}^T C^{-\frac{1}{2}} = U \tilde{\Sigma} V^T$$
$\mathbf{(b)}$ By $(a)$, we have $A = \sum_j \tilde{\sigma}_j \cdot (B^{-\frac{1}{2}} \tilde{u_j}) \cdot (C^{-\frac{1}{2}} \tilde{v_j})^T$
\end{proof}

\begin{question}[5]
\end{question}
\begin{proof}
By contradiction, suppose we can only find $m$ points $\{s_j\}_{1 \le j \le m}$ for which the columns of $\{f(s_j)\}_{1 \le j \le m}$ are linearly independent and let $A_S = \begin{pmatrix}
f(s_1) & f(s_2) & \ldots & f(s_m)
\end{pmatrix}$ be $n \times m,\ m < n$. Then $A_S$ has linearly dependent rows since we have a collection of $n$ (row) vectors in $\mathbb{R}^m$. So there exists a nontrivial coefficient vector $\alpha \in \mathbb{R}^n$, such that $\alpha^T A_S = 0 \in \mathbb{R}^{1 \times m}$. Let us expand the last expression to see why this is an immediate contradiction --
$$\begin{pmatrix}
0 & 0 & \ldots & 0
\end{pmatrix} = \alpha^T A_S = \begin{pmatrix}
\alpha_1 & \alpha_2 & \ldots & \alpha_n
\end{pmatrix} \cdot \begin{pmatrix}
f_1(s_1) & f_1(s_2) & \ldots & f_1(s_m) \\
f_2(s_1) & f_2(s_2) & \ldots & f_2(s_m) \\
\vdots & \vdots & \ddots & \vdots \\
f_n(s_1) & f_n(s_2) & \ldots & f_n(s_m)
\end{pmatrix} $$
$$\iff \sum_{i=1}^n \alpha_i \cdot f_i(s_j) = 0$$
where $\alpha_i \ne 0$ for at least one $i$. This contradicts the linear independence of $\{f_i\}$.
\end{proof}

\begin{question}[6]
\end{question}
\begin{proof}
By contradiction, suppose we can only find $m$ points $\{s_j\}_{1 \le j \le m}$ for which the columns of $\{f(s_j)\}_{1 \le j \le m}$ are linearly independent and let $A_S = \begin{pmatrix}
f(s_1) & f(s_2) & \ldots & f(s_m)
\end{pmatrix}$ be $n \times m,\ m < n$. Then $A_S$ has linearly dependent rows since we have a collection of $n$ (row) vectors in $\mathbb{R}^m$. So there exists a nontrivial coefficient vector $\alpha \in \mathbb{R}^n$, such that $\alpha^T A_S = 0$. A right action by $A_S$ implies a linear combination of its rows. Let $(\alpha^T A_S)_j$ denote the $j^{th}$ entry of the row vector $\alpha^T A_S$. Let 
$$f = \begin{pmatrix}
f_1 \\
\vdots \\
f_n
\end{pmatrix},\ f(s_j) = \begin{pmatrix}
f_1(s_j) \\
\vdots \\
f_n(s_j)
\end{pmatrix}$$
where $f(s_j)$ denotes the column of $A_S$ at $s_j$.
\\Let $i$ be such that $\alpha_i \ne 0$. Then,
$$(\alpha^T A_S)_i = \langle \alpha^T, f(s_i) \rangle = \sum_{i=1}^n \alpha_i \cdot f_i(s_j) = 0$$
contradicting the linear independence of $\{f_i\}$.
\end{proof}

\end{document}
