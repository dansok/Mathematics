\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\addtolength{\evensidemargin}{-.5in}
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\textwidth}{0.8in}
\addtolength{\textheight}{0.8in}
\addtolength{\topmargin}{-.4in}
\newtheoremstyle{quest}{\topsep}{\topsep}{}{}{\bfseries}{}{ }{\thmname{#1}\thmnote{ #3}.}
\theoremstyle{quest}
\newtheorem*{definition}{Definition}
\newtheorem*{theorem}{Theorem}
\newtheorem*{question}{Question}
\newtheorem*{exercise}{Exercise}
\newtheorem*{challengeproblem}{Challenge Problem}
\newcommand{\name}{%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% put your name here, so we know who to give credit to %%
Dan Sokolsky
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\hw}{%%%%%%%%%%%%%%%%%%%%
%% and which homework assignment is it? %%%%%%%%%
%% put the correct number below              %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
2
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\vspace{-50pt}
\Huge \name
\\\vspace{20pt}
\huge Linear Algebra II\hfill Homework \hw}
\author{}
\date{}
\pagestyle{myheadings}
\markright{\name\hfill Homework \hw\qquad\hfill}

%% If you want to define a new command, you can do it like this:
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

%% If you want to use a function like ''sin'' or ''cos'', you can do it like this
%% (we probably won't have much use for this)
% \DeclareMathOperator{\sin}{sin}   %% just an example (it's already defined)


\begin{document}
\maketitle

\begin{question}[1]
Prove the minimal polynomial of the $n \times n$ matrix $A$, $m_A$, must include all distinct eigenvalues of $A$.
\end{question}
\begin{proof}
$(\lambda, v)$ an eigen-pair of $A \implies m(\lambda)v = m(A)v = 0 \cdot v = 0$, since $A^k v = \lambda^k v$. Since $v \ne 0$, it follows that $m(\lambda) = 0$.
\end{proof}
\begin{question}[2]
Given that the distinct roots of $m_A$ are the distinct eigenvalues of the $n \times n$ matrix $A$, show $-$
\\(a) $d_j \le m_j$ where $m_j$ is the algebraic multiplicity of the eigenvalue $a_j$, and $d_j$ is the multiplicity of $a_j$ as a root of  $m_A$.
\\(b) $d_j$ is the index of the eigenvalue $a_j$.
\end{question}
\begin{proof}
(a) Suppose $d_j > m_j$. Then $(t - a_j)^{d_j} \nmid (t - a_j)^{m_j}$. Thus $m_A \nmid p_A$, a contradiction.
\\(b) Let $m_A(t) = \prod_j(t-a_j)^{d_j}$,  let $p_A(t) = \prod_j(t-a_j)^{m_j}$,  and define $N_j := N_{(A-a_jI)^{m_j}}$. Then $(t-a_i)^{m_i}$ and $(t-a_j)^{m_j}$ are coprime for $i \ne j$, and by the Spectral Theorem, $\mathbb{C}^n = N_0 = N_{p_A(A)} = \bigoplus_j N_j$. Thus $0_{n \times n} = m_A(A) \iff \forall j, \ 0 = m_A(A) \restriction_{N_j} = (A - a_jI)^{d_j} \restriction_{N_j} \iff (A - a_jI)^{d_j} = 0 \iff d_j$ is the index of the eigenvalue $a_j$, as the minimal power such that $(A - a_jI)^{d_j} = 0$, satisfying the minimality requirement of the degree of $m_A$, as the minimal polynomial.
\end{proof}
\begin{question}[3]
Let $A$ be $n \times n$. Use a theorem in Chapter 3 to show that $\mathbb{C}^n = R_A \oplus N_A$
\end{question}
\begin{proof}
This is false. Consider the $2 \times 2$ matrix $A = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}$. Then $A \cdot \begin{bmatrix} 1\\ 0 \end{bmatrix} = \begin{bmatrix} 0\\ 0 \end{bmatrix}$, and $A \cdot \begin{bmatrix} 0\\ 1 \end{bmatrix} = \begin{bmatrix} 1\\ 0 \end{bmatrix}$. That is, $N_A = span\Big\{\begin{bmatrix} 1\\ 0 \end{bmatrix}\Big\} = \mathbb{C} \times \{0\}$, and $R_A = span\Big\{\begin{bmatrix} 1\\ 0 \end{bmatrix}\Big\} = \mathbb{C} \times \{0\}$. Then $R_A + N_A = \mathbb{C} \times \{0\} \ne \mathbb{C} \times \mathbb{C} = \mathbb{C}^2$.
\end{proof}
\begin{question}[4]
Let $A$ be $n \times n$ of rank 2. List all possible forms of the minimal polynomial of $A$.
\end{question}
\begin{proof}
$m_A(t) = (t-a)$
\end{proof}
\begin{question}[5]
Exercise 11 page 76
\end{question}
\begin{proof}
Let $A = \begin{bmatrix} 0 & 1 \\ 1 & 1 \end{bmatrix}$. Then $A^T = A$.
\\(a) The two eigenvalues are $a_1 = \frac{1 + \sqrt{5}}{2}$, and $a_2 = \frac{1 - \sqrt{5}}{2}$. We solve $A^Tv = c v \ -$ $$\begin{bmatrix} v_2 \\ v_1 + v_2 \end{bmatrix} = \begin{bmatrix} 0 & 1 \\ 1 & 1 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = c \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = \begin{bmatrix} c v_1 \\ c v_2 \end{bmatrix} \iff v_2 = c v_1\  \& \ v_1 + v_2 = c v_2$$ $$\iff v_1 + c v_1 = c^2 v_1 \iff c^2 v_1 - c v_1 - v_1 = v_1(c^2 - c - 1) = 0 \\ \ \ (*)$$ where $a_1, a_2$ are precisely the solutions for $c^2 - c - 1 = 0$. Thus $(*)$ always holds. Therefore $h_1 = \begin{bmatrix} 1 \\ a_1 \end{bmatrix}$ is the eigenvector for $a_1$. Likewise, $h_2 = \begin{bmatrix} 1 \\ a_2 \end{bmatrix}$ is the eigenvector for $a_2$.
\\(b) Note $(h_1, h_1) = 1 + a_1^2 = \frac{5+\sqrt{5}}{2}$. $(h_2, h_2) = 1 + a_2^2 = \frac{5-\sqrt{5}}{2}$. For $v = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$, $(h_1, v) = a_1$, and $(h_2, v) = a_2$. By formulas $(44)$ and $(45)$, $v = k_1h_1 + k_2h_2$; with $k_1 = \frac{a_1}{\frac{5 + \sqrt{5}}{2}} = \frac{1}{\sqrt{5}}$, and $k_2 = \frac{a_2}{\frac{5 - \sqrt{5}}{2}} = -\frac{1}{\sqrt{5}}$. Indeed, this agrees with the expansion in Example $2$.
\end{proof}

\end{document}
