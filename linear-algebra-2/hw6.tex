\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\addtolength{\evensidemargin}{-.5in}
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\textwidth}{0.8in}
\addtolength{\textheight}{0.8in}
\addtolength{\topmargin}{-.4in}
\newtheoremstyle{quest}{\topsep}{\topsep}{}{}{\bfseries}{}{ }{\thmname{#1}\thmnote{ #3}.}
\theoremstyle{quest}
\newtheorem*{definition}{Definition}
\newtheorem*{theorem}{Theorem}
\newtheorem*{question}{Question}
\newtheorem*{exercise}{Exercise}
\newtheorem*{challengeproblem}{Challenge Problem}
\newcommand{\name}{%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% put your name here, so we know who to give credit to %%
Dan Sokolsky
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\hw}{%%%%%%%%%%%%%%%%%%%%
%% and which homework assignment is it? %%%%%%%%%
%% put the correct number below              %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
6
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\vspace{-50pt}
\Huge \name
\\\vspace{20pt}
\huge Linear Algebra II\hfill Homework \hw}
\author{}
\date{}
\pagestyle{myheadings}
\markright{\name\hfill Homework \hw\qquad\hfill}

%% If you want to define a new command, you can do it like this:
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

%% If you want to use a function like ''sin'' or ''cos'', you can do it like this
%% (we probably won't have much use for this)
% \DeclareMathOperator{\sin}{sin}   %% just an example (it's already defined)


\begin{document}
\maketitle

\begin{question}[1]
Prove there exists a unique polynomial $q$ such that $A^{-1} = q(A)$.
\end{question}
\begin{proof}
Suppose $A$ is invertible. Let $m_A(t) = \sum_{i = 0}^k c_i \cdot t^i$ be the minimal polynomial of $A$. Then,
$$m_A(A) = A^k + c_{k-1} A^{k-1} + \ldots + c_1 A + c_0 I = 0$$
If $c_0 = 0$, then it follows
$$(A^{k-1} + c_{k-1}A^{k-2} + \ldots + c_1 I)A = 0\ \ \ \ \ \ \ \ \ (1)$$
Setting $s(A) = A^{k-1} + c_{k-1}A^{k-2} + \ldots + c_1 I$, we have $s(A) \ne 0$, since $\deg s < \deg m_A$. \\By $(1)$, $A$ can be reduced to $0_{n \times n}$ by nontrivial row operations, implying $A$ is not invertible, a contradiction. So $c_0 \ne 0$. Now,
$$0 = m_A(A) = A^k + c_{k-1} A^{k-1} + \ldots + c_1 A + c_0 I $$
$$\iff (A^{k-1} + c_{k-1}A^{k-2} + \ldots + c_1 I)A = -c_0 I = A(A^{k-1} + c_{k-1}A^{k-2} + \ldots + c_1 I)\ \ \ \ \ \ \ \ \ (2)$$
Set $$q(A) = -\frac{1}{c_0} s(A) = -\frac{1}{c_0} (A^{k-1} + c_{k-1}A^{k-2} + \ldots + c_1 I)$$
By $(2)$, we see that
$$q(A) \cdot A = I = A \cdot q(A)$$
Thus $q(A) = A^{-1}$.\\
To prove uniqueness, let $p(t)$ be another polynomial such that $p(A) = A^{-1} = q(A)$. By contradiction, suppose $q(t) - p(t) \ne 0$, and WLOG, $\deg q \ge \deg p$. \\Define $h(t) = q(t) - p(t)$ and observe $h(A) = q(A) - p(A) = A^{-1} - A^{-1} = 0$, but $\deg h \le \deg q < \deg m_A$, a contradiction. Thus $q(t)$ is unique.
\end{proof}
\begin{question}[2]
Let $A$ be $n \times n$ be diagonalizable with $k$ distinct eigenvalues $\{a_j\}_{1 \le j \le k}$. Let $p(t)$ be a polynomial that interpolates $f(t) = t^{-1}$ at $a_j$ with $\deg p = k-1$. Show that $p = q$ where $A^{-1} = q(A)$.
\end{question}
\begin{proof}
First, note $f(a_j) = \frac{1}{a_j}$. So for $f$ to be well defined, and $p$ to interpolate it at $a_j$, we must have $a_j \ne 0$. Let A be diagonalizable and let J be it's Jordan Normal Form \\(J is a diagonal matrix in this case). Then $A = SJS^{-1}$. \\Now, $p(a_j) = f(a_j) = \frac{1}{a_j}$, and
$$p(J) = \begin{pmatrix}
p(a_1) & \cdots & 0 & \cdots & 0 \\
\vdots & \ddots & \vdots & \ddots  & 0 \\
0 & \cdots & p(a_j) & \cdots & 0 \\
\vdots & \ddots & \vdots & \ddots & 0 \\
0 & \cdots & 0 & \cdots & p(a_k)
\end{pmatrix} = \begin{pmatrix}
\frac{1}{a_1} & \cdots & 0 & \cdots & 0 \\
\vdots & \ddots & \vdots & \ddots  & 0 \\
0 & \cdots & \frac{1}{a_j} & \cdots & 0 \\
\vdots & \ddots & \vdots & \ddots & 0 \\
0 & \cdots & 0 & \cdots & \frac{1}{a_k}
\end{pmatrix} = J^{-1}$$
Since by question $(1)$ the polynomial $q(t)$ is unique, it follows that $p(t) = q(t)$ in the case of diagonal matrices. For the general case, observe --
$$p(A) = p(SJS^{-1}) = S\cdot p(J)\cdot S^{-1} = S\cdot q(J)\cdot S^{-1} = q(SJS^{-1}) = q(A)$$
where we have shown that for an arbitrary polynomial $r(SAS^{-1}) = S\cdot r(J)\cdot S^{-1}$ in homework $5$. Once again, by uniqueness of the polynomial $q(t)$, we conclude $p(t) = q(t)$ in the general case.
\end{proof}
\begin{question}[3]
Let $A$ be a super-diagonal $n \times n$ Jordan block, $n > 3$, with eigenvalue $a \ne 0$. Give a constructive algorithm to find $A^{-1}$.
\end{question}
\begin{proof}
We use the algorithm outlined in question $(1)$.
$$m_A(t) = (t-a)^n = \sum_{k=0}^n \binom{n}{k} \cdot t^k \cdot (-a)^{n-k} = (-a)^n + \sum_{k=1}^n \binom{n}{k} \cdot t^k \cdot (-a)^{n-k}$$
Thus,
$$0 = m_A(A) = (-a)^nI + \sum_{k=1}^n \binom{n}{k} \cdot A^k \cdot (-a)^{n-k}$$
$$\iff -(-a)^nI = A \cdot \sum_{k=0}^{n-1} \binom{n}{k+1} (-a)^{n-k-1} A^k$$
$$\iff A^{-1} = \frac{-1}{(-a)^n} \cdot \sum_{k=0}^{n-1} \binom{n}{k+1} (-a)^{n-k-1} A^k = - \sum_{k=0}^{n-1} \binom{n}{k+1}(-a)^{-k-1} A^k$$
\end{proof}
\begin{question}[4]
Let $X_0 = \frac{1}{4}I$. For $k > 0$, let $X_{k+1} = X_k(2I - AX_k)$.
\\(a) Let $\epsilon_k = ||I - AX_k||$. Show $\epsilon_{k+1} = \epsilon_k^2$
\\(b) What can you say of $X_k$ as a polynomial of $A$?
\end{question}
\begin{proof}
(a) $$\epsilon_{k+1} = ||I - AX_{k+1}|| = ||I - AX_k(2I-AX_k)|| = ||I - 2AX_k + (AX_k)^2||$$ $$= ||(I - AX_k)^2|| \le ||I - AX_k||^2 = \epsilon_k^2$$
I.e. $\epsilon_{k+1} = O(\epsilon_k^2)$. This suffices to prove quadratic rate of convergence.
\\$\epsilon_{k+1} \not\ge \epsilon_k^2$ in general. Observe, $$\epsilon_0 = ||I - AX_0|| = ||I - \frac{1}{4}A||$$and$$\epsilon_1 = ||I - AX_1|| = ||I - \frac{1}{2}A + \frac{1}{16}A^2|| = ||(I - \frac{1}{4}A)^2||$$
Taking $A = 4I - 4 \begin{pmatrix}
0 & 2 \\
1 & 3
\end{pmatrix}$, we have $B := I - \frac{1}{4}A = \begin{pmatrix}
0 & 2 \\
1 & 3
\end{pmatrix}$. The matrix $A$ is invertible, with $A^{-1} = \frac{1}{16}\begin{pmatrix}
2 & -2 \\
-1 & -1
\end{pmatrix}$.
\\Thus, by Wolfram, $$\epsilon_0^2 = ||B||^2 = 7 + 3\sqrt{5} \approx 13.7082 > 13.0348 \approx \frac{9 + \sqrt{89}}{\sqrt{2}} = ||B^2|| = \epsilon_1$$
\\(b) $X_k$ grows exponentially as a polynomial of $A$, both in degree, and in the number of terms. We see That $X_0 = \frac{1}{4}I = O(I)$, $X_1 = \frac{1}{2}I - \frac{1}{16}A = O(A)$, \\$X_2 = -A^3/256 + A^2/16 - (3 A)/8 + I = O(A^3)$, \\$X_3 = -A^7/65536 + A^6/2048 - (7 A^5)/1024 + (7 A^4)/128 - (35 A^3)/128 + (7 A^2)/8 - (7 A)/4 + 2I = O(A^7)$, \\$X_4 = O(A^{15})$. In general, $X_k = O(A^{2^k-1})$, and we also see that at each step the number of terms at least doubles, yielding exponential growth in terms as well.
\end{proof}
\begin{question}[5]
Let $(a_0, x_0)$ be an eigen-pair of $A$. Let $x \ne 0$ be closest to $x_0$ of all the eigenvectors. Let $\delta = ||x - x_0||$
\\(a) Solve the least squares problem $a = \arg\min_\lambda ||Ax - x
\lambda||^2$.
\end{question}
\begin{proof}
(a) We claim $a = \dfrac{x^*Ax}{x^*x}$ and proceed to verify equation $(25)$ holds, for which the solution is unique. Substituting $x$ as our matrix, and $Ax$ as our vector $p$,
$$x^*x \cdot a = x^*x \cdot \dfrac{x^*Ax}{x^*x} = x^*Ax$$
as required. Thus $a$ is the unique solution to equation $(25)$, and is the desired approximate eigenvalue.
\end{proof}

\end{document}
