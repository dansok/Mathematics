\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\addtolength{\evensidemargin}{-.5in}
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\textwidth}{0.8in}
\addtolength{\textheight}{0.8in}
\addtolength{\topmargin}{-.4in}
\newtheoremstyle{quest}{\topsep}{\topsep}{}{}{\bfseries}{}{ }{\thmname{#1}\thmnote{ #3}.}
\theoremstyle{quest}
\newtheorem*{definition}{Definition}
\newtheorem*{theorem}{Theorem}
\newtheorem*{question}{Question}
\newtheorem*{exercise}{Exercise}
\newtheorem*{challengeproblem}{Challenge Problem}
\newcommand{\name}{%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% put your name here, so we know who to give credit to %%
Dan Sokolsky
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\hw}{%%%%%%%%%%%%%%%%%%%%
%% and which homework assignment is it? %%%%%%%%%
%% put the correct number below              %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
6
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\vspace{-50pt}
\Huge \name
\\\vspace{20pt}
\huge Linear Algebra II\hfill Homework \hw}
\author{}
\date{}
\pagestyle{myheadings}
\markright{\name\hfill Homework \hw\qquad\hfill}

%% If you want to define a new command, you can do it like this:
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

%% If you want to use a function like ''sin'' or ''cos'', you can do it like this
%% (we probably won't have much use for this)
% \DeclareMathOperator{\sin}{sin}   %% just an example (it's already defined)


\begin{document}
\maketitle

\begin{question}[1]
Prove there exists a unique polynomial $q$ such that $A^{-1} = q(A)$.
\end{question}
\begin{proof}
Suppose $A$ is invertible. Let $m_A(t) = \sum_{i = 0}^k c_i \cdot t^i$ be the minimal polynomial of $A$. Then,
$$m_A(A) = A^k + c_{k-1} A^{k-1} + \ldots + c_1 A + c_0 I = 0$$
If $c_0 = 0$, then it follows
$$A(A^{k-1} + c_{k-1}A^{k-2} + \ldots + c_1 I) = 0\ \ \ \ \ \ \ \ \ (1)$$
Setting $s(A) = A^{k-1} + c_{k-1}A^{k-2} + \ldots + c_1 I$, we have $s(A) \ne 0$, since $\deg s < \deg m_A$. By $(1)$, $A$ can be column reduced to $0_{n \times n}$, implying $A$ is not invertible, a contradiction. So $c_0 \ne 0$. Now,
$$0 = m_A(A) = A^k + c_{k-1} A^{k-1} + \ldots + c_1 A + c_0 I $$
$$\iff (A^{k-1} + c_{k-1}A^{k-2} + \ldots + c_1 I)A = -c_0 I = A(A^{k-1} + c_{k-1}A^{k-2} + \ldots + c_1 I)\ \ \ \ \ \ \ \ \ (2)$$
Set $$q(A) = -\frac{1}{c_0} s(A) = -\frac{1}{c_0} (A^{k-1} + c_{k-1}A^{k-2} + \ldots + c_1 I)$$
By $(2)$, we see that
$$q(A) \cdot A = I = A \cdot q(A)$$
Thus $q(A) = A^{-1}$.\\
To prove uniqueness, let $p(t)$ be another polynomial such that $p(A) = A^{-1} = q(A)$. By contradiction, suppose $q(t) - p(t) \ne 0$, and WLOG, $\deg q \ge \deg p$. \\Define $h(t) = q(t) - p(t)$ and observe $h(A) = q(A) - p(A) = A^{-1} - A^{-1} = 0$, but $\deg h \le \deg q < \deg m_A$, a contradiction. Thus $q(t)$ is unique.
\end{proof}
\begin{question}[2]
Let $A$ be $n \times n$ be diagonalizable with $k$ distinct eigenvalues $\{a_j\}_{1 \le j \le k}$. Let $p(t)$ be a polynomial that interpolates $f(t) = t^{-1}$ at $a_j$ with $\deg p = k-1$. Show that $p = q$ where $A^{-1} = q(A)$.
\end{question}
\begin{proof}
First, note $f(a_j) = \frac{1}{a_j}$. So for $f$ to be well defined, and $p$ to interpolate it at $a_j$, we must have $a_j \ne 0$. Let A be diagonalizable and let J be it's Jordan Normal Form (J is a diagonal matrix in this case). Then $A = SJS^{-1}$. Now,
$$\det J^{-1} = \frac{1}{\det J} = \prod_{j=1}^k (\frac{1}{a_j})^{m_j}$$
$m_j$ being the multiplicity of the eigenvalue $a_j$. So the eigenvalues of $J^{-1}$ are $\{\frac{1}{a_j}\}_{1 \le j \le k}$. Now, $p(a_j) = f(a_j) = \frac{1}{a_j}$, and
$$p(J) = \begin{pmatrix}
p(a_1) & \cdots & 0 & \cdots & 0 \\
\vdots & \ddots & \vdots & \ddots  & 0 \\
0 & \cdots & p(a_j) & \cdots & 0 \\
\vdots & \ddots & \vdots & \ddots & 0 \\
0 & \cdots & 0 & \cdots & p(a_k)
\end{pmatrix} = \begin{pmatrix}
\frac{1}{a_1} & \cdots & 0 & \cdots & 0 \\
\vdots & \ddots & \vdots & \ddots  & 0 \\
0 & \cdots & \frac{1}{a_j} & \cdots & 0 \\
\vdots & \ddots & \vdots & \ddots & 0 \\
0 & \cdots & 0 & \cdots & \frac{1}{a_k}
\end{pmatrix} = J^{-1}$$
Since by question $(1)$, the polynomial $q$ is unique, it follows that $p(t) = q(t)$ in the case of diagonal matrices. For the general case, observe --
$$p(A) = p(SJS^{-1}) = S\cdot p(J)\cdot S^{-1} = S\cdot q(J)\cdot S^{-1} = q(SJS^{-1}) = q(A)$$
where we have shown that for an arbitrary polynomial $r(SAS^{-1}) = S\cdot r(J)\cdot S^{-1}$ in homework $5$. Once again, by uniqueness of the polynomial $q$, we conclude $p(t) = q(t)$ in the general case.
\end{proof}

\end{document}
