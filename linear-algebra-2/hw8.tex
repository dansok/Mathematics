\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm, mathrsfs}
\addtolength{\evensidemargin}{-.5in}
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\textwidth}{0.8in}
\addtolength{\textheight}{0.8in}
\addtolength{\topmargin}{-.4in}
\newtheoremstyle{quest}{\topsep}{\topsep}{}{}{\bfseries}{}{ }{\thmname{#1}\thmnote{ #3}.}
\theoremstyle{quest}
\newtheorem*{definition}{Definition}
\newtheorem*{theorem}{Theorem}
\newtheorem*{question}{Question}
\newtheorem*{exercise}{Exercise}
\newtheorem*{challengeproblem}{Challenge Problem}
\newcommand{\name}{%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% put your name here, so we know who to give credit to %%
Dan Sokolsky
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\hw}{%%%%%%%%%%%%%%%%%%%%
%% and which homework assignment is it? %%%%%%%%%
%% put the correct number below              %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
8
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\vspace{-50pt}
\Huge \name
\\\vspace{20pt}
\huge Linear Algebra II\hfill Homework \hw}
\author{}
\date{}
\pagestyle{myheadings}
\markright{\name\hfill Homework \hw\qquad\hfill}

%% If you want to define a new command, you can do it like this:
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

%% If you want to use a function like ''sin'' or ''cos'', you can do it like this
%% (we probably won't have much use for this)
% \DeclareMathOperator{\sin}{sin}   %% just an example (it's already defined)


\begin{document}
\maketitle

\begin{question}[1]
(a) Let $A, B \in \mathbb{R}^{m \times n}$. Prove $tr(B^TA) = vec(A) \cdot vec(B)$.
\\(b) Prove $||A + B||_F \le ||A||_F + ||B||_F$
\end{question}
\begin{proof}
$\mathbf{(a)}$ Observe that the $i^{th}$ row of $B^T$ is the $i^{th}$ column of $B$. Further observe $B^TA$ is $n \times n$. Denote by $a_j,\ b_j$ the $j^{th}$ columns of $A, B$, respectively.
\\Thus, $(B^TA)_{i j} = \langle b_i, a_j \rangle$. Therefore,
$$tr(B^TA) = \sum_{j=1}^n \langle a_j, b_j \rangle = \sum_{i=1}^m \sum_{j=1}^n A_{i j} \cdot B_{i j} = \langle vec(A), vec(B)\rangle$$
\\$\mathbf{(b)}$
$$||A+B||_F = ||vec(A+B)||_2 = ||vec(A) + vec(B)||_2$$ $$\le ||vec(A)||_2 + ||vec(B)||_2 = ||A||_F + ||B||_F$$
\end{proof}
\begin{question}[2]
(a) Prove $||A^TA||_F \le ||A||_F^2$
\\(b) Let $A$ be $m \times k$. Let $B$ be $k \times n$. Prove $||AB||_F \le ||A||_F||B||_F$
\end{question}
\begin{proof}
$\mathbf{(a)}$ Let $a_j$ denote the $j^{th}$ column of $A$. As before, the $i^{th}$ row of $A^T$ is the $i^{th}$ column of $A$. Thus, $(A^TA)_{i j} = \langle a_i, a_j \rangle$. Thus,
$$||A^TA||_F = \sqrt{\sum_{1 \le i,j \le n} \langle a_i, a_j \rangle^2} \le \sqrt{\sum_{1 \le i,j \le n} ||a_i||^2 \cdot ||a_j||^2} = $$ $$\sqrt{\sum_{i=1}^n \sum_{j=1}^n ||a_i||^2 \cdot ||a_j||^2} = \sqrt{\sum_{i=1}^n ||a_i||^2 \cdot \sum_{j=1}^n  ||a_j||^2} = ||A||_F \cdot ||A||_F = ||A||_F^2$$
\\$\mathbf{(b)}$ First we note
$$||BB^T||_F^2 = tr((BB^T)^TBB^T) = tr(BB^TBB^T) =$$
$$tr(B^TBB^TB) = tr((B^TB)^T(B^TB)) = ||B^TB||_F^2$$
$\iff ||BB^T||_F = ||B^TB||_F$. Then,
$$||AB||_F^2 = tr((AB)^T(AB)) = tr(B^T A^T A B) = tr((BB^T)^T(A^TA))$$
$$= \langle vec(BB^T), vec(A^TA) \rangle \le ||vec(BB^T)||_2 \cdot ||vec(A^TA)||_2 = $$
$$||BB^T||_F \cdot ||A^TA||_F = ||B^TB||_F \cdot ||A^TA||_F \le ||B||_F^2 \cdot ||A||_F^2$$
$\iff ||AB||_F \le ||A||_F \cdot ||B||_F$
\end{proof}
\begin{question}[3]
Find a best approximation to $A,\ R_1 = min_{rank(R)=1} ||A-R||_F$
\end{question}
\begin{proof}
First, note that by definition of the two norms, for $x,y \in \mathbb{R}^d,\ ||x||_2 = ||x||_F$ and $\langle x,y \rangle_2 = \langle x,y \rangle_F$. Thus question $(5)$ in homework $7$ applies for the Frobenius norm as well.
$$||A-R||_F^2 = ||A-R_1+R_1-R||_F^2 = ||vec(A-R_1+R_1-R)||_2^2 = ||vec(A-R_1)+vec(R_1-R)||_2^2$$
$$=||vec(A-R_1)||_2^2 + 2\langle vec(A-R_1), vec(R_1-R) \rangle + ||vec(R_1-R)||_2^2 =$$
$$||A-R_1||_F^2 + 2tr((A-R_1)^T(R_1-R)) + ||R_1-R||_F^2 =$$
$$||A-R_1||_F^2 + 2\langle A-R_1, R_1-R\rangle_F + ||R_1-R||_F^2$$
Thus,
$$||A-R||_F^2 \ge ||A-R_1||_F^2 \iff 2\langle A-R_1, R_1-R\rangle_F + ||R_1-R||_F^2 \ge 0$$
Now, as in problem $(5)$ in homework $7$, for $R = uv^T$ to be arbitrarily close to $R_1 = ab^T$, $R$ must be a constant multiple of $R_1$. Setting $R = (1-t)R_1$, as before, we must have $\langle A-R_1, R_1 \rangle_F = 0$ as a necessary condition.
\end{proof}

\begin{question}[4]
Let $A \in \mathbb{R}^{m \times n}$. Let $rank(A) = k$. Prove there exists a decomposition $A = UV^T;\ U \in \mathbb{R}^{m \times k},\ V \in \mathbb{R}^{n \times k}$.
\end{question}
\begin{proof}
Let $\{u_1, \ldots, u_k\}$ be a basis for $R_A$, $u_i \in \mathbb{R}^m$. Then the $j^{th}$ column of $A$, denoted $a_j$, can be written --
$$a_j = v_{1j} \cdot u_1 + v_{2j} \cdot u_2 + \ldots + v_{kj} \cdot u_k;\ \ \ \ \ \ \ \ 1 \le j \le n\ \ \ \ \ \ \ \ \ \ (8.1)$$
Let $U = \begin{pmatrix}
u_1\ | & u_2\ | & \ldots\ | & u_k
\end{pmatrix}$. Then $U \in \mathbb{R}^{m \times k}$. Define $V \in \mathbb{R}^{n \times k}$ by $V_{ji} = v_{ij}$ where the coefficients $v_{ij}$ are coming from $(8.1)$. Then $V^T \in \mathbb{R}^{k \times n}$ and from $(8.1)$ we see that
$$A \cdot e_j = a_j = (UV^T) \cdot e_j \iff A = UV^T$$
\end{proof}
\begin{question}
Let $A \in \mathbb{R}^{m \times n}$. Let $rank(A) = k$. Let $v \not \in N_A$ be a unit vector such that $u = Av \ne 0$. Let $A_1 = A - uv^T$.
\\(a) Show $vec(A_1) \cdot vec(uv^T) = 0$.
\\(b) Show $rank(A_1) = k-1$.
\end{question}
\begin{proof}
$\mathbf{(a)}$ We note that the Frobenius inner-product is bi-additive, since for matrices $X, Y, Z \in \mathbb{R}^{m \times n}$, $(X+Y)_{ij} \cdot Z_{ij} = (X_{ij} + Y_{ij}) \cdot Z_{ij} = X_{ij} \cdot Z_{ij} + Y_{ij} \cdot Z_{ij}$. The Frobenius inner-product is also a bi-linear form over $\mathbb{R}$ (it is sequilinear over $\mathbb{C}$). Further, recall -- $||v||_F^2 = ||v||_2^2 = 1 \iff \sum_{j=1}^n v_j^2 = 1$. Thus,
$$vec(A_1) \cdot vec(uv^T) = \langle A_1, uv^T \rangle_F = \langle A - Avv^T, Avv^T \rangle_F = \langle A, Avv^T \rangle_F - \langle Avv^T, Avv^T \rangle_F$$
Thus, suffices to prove $ \langle A, Avv^T \rangle_F = \langle Avv^T, Avv^T \rangle_F$. To this end, Observe --
$$(vv^T)_{ij} = v_i \cdot v_j$$
and therefore
$$(uv^T)_{ij} = (Avv^T)_{ij} = \sum_{l=1}^n A_{il} \cdot v_l \cdot v_j$$
Thus,
$$\langle A, Avv^T \rangle_F = \sum_{i =1}^m \sum_{j=1}^n A_{ij} \cdot (Avv^T)_{ij} = \sum_{i =1}^m \sum_{j=1}^n A_{ij} \cdot \sum_{l=1}^n A_{il} \cdot v_l \cdot v_j =$$ $$\sum_{i =1}^m (\sum_{j=1}^n A_{ij} \cdot v_j) \cdot (\sum_{l=1}^n A_{il} \cdot v_l) = \sum_{i =1}^m (\sum_{j=1}^n A_{ij} \cdot v_j)^2$$
and
$$\langle Avv^T, Avv^T \rangle_F = \sum_{i =1}^m \sum_{j =1}^n (Avv^T)_{ij}^2 = \sum_{i =1}^m \sum_{j =1}^n (\sum_{l=1}^n A_{il} \cdot v_l \cdot v_j)^2 = \sum_{i =1}^m \sum_{j =1}^n v_j^2 \cdot (\sum_{l=1}^n A_{il} \cdot v_l)^2$$
$$= \sum_{i =1}^m 1 \cdot (\sum_{l=1}^n A_{il} \cdot v_l)^2 = \sum_{i =1}^m (\sum_{j=1}^n A_{ij} \cdot v_j)^2$$
\\$\mathbf{(b)}$ Since $v \ne 0$ as a unit vector, and $u = Av \ne 0$, we can extend $\{u\}$ to form a basis for $R_A$ such that
$$a_j = y_{1j} \cdot x_1 + y_{2j} \cdot x_2 + \ldots + y_{(k-1)j} \cdot x_{k-1} + v_j \cdot u_k\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (8.2)$$
where $\{x_i\}_{1 \le i \le k-1}$ is linearly independent. As in the Rank Decomposition Theorem (question $4$), it follows that
$$A = \sum_{i=1}^{k-1} x_i y_i^T + uv^T$$
It follows that
$$A_1 = A - uv^T = \sum_{i=1}^{k-1} x_i y_i^T + uv^T - uv^T = \sum_{i=1}^{k-1} x_i y_i^T$$
By $(8.2)$, this means that the column space of $A_1$ is spanned by the $k-1$ linearly independent vectors $\{x_i\}_{1 \le i \le k-1}$. It follows that $rank(A_1) = k-1$.
\end{proof}
\begin{question}[6]
(a) Let $\{u_i\}_{1 \le i \le p}$ be linearly independent, and let $\{v_i\}_{1 \le i \le p}$ be such that $v_i \ne 0$. What can be said of $u_iv_i^T$ as vectors in $\mathbb{R}^{m \times n}$?
\\(b) What can be said of the apparent discrepancy of the dimensions?
\end{question}
\begin{proof}
$\mathbf{(a)}$ As seen from $(8.1)$, since $\{u_i\}_{1 \le i \le p}$ is linearly independent, it follows that it forms a basis for rank-$p$ matrices in $\mathbb{R}^{m \times n}$ w.r.t the outer product with the set of weight vectors $\{v_i\}_{1 \le i \le p}$. Meaning, given a matrix
$A \in \mathbb{R}^{m \times n}$ with $rank(A)=p$, we can find a set of vectors $\{v_i\}_{1 \le i \le p}$ such that $\sum_{i=1}^p u_i v_i^T = A$.
\\$\mathbf{(b)}$ WLOG, suppose $m \ge n$. I.e., our matrices are tall. For vectors in $\mathbb{R}^{mn}$, the underlying field that the vector space is defined over is $\mathbb{R}$. That is, you need $m \cdot n$ coefficients and $m \cdot n$ vectors to span all of $\mathbb{R}^{mn}$. However, for $m \times n$ matrices seen as vectors in $\mathbb{R}^{m \times n}$, the underlying field for the vector space is $\mathbb{R}^m$. That is, suffices to pick $n$ linearly independent vectors $u_i,\ u_i \in \mathbb{R}^m$; and $n$ weight vectors $v_i,\ v_i \in \mathbb{R}^m$ to span all matrices of rank $\le n$ in $\mathbb{R}^{m \times n}$ (that's all the matrices in $\mathbb{R}^{m \times n}$). As seen in $(8.1)$, and again in $\mathbf{(a)}$, that means that for a given matrix $A \in \mathbb{R}^{m \times n}$, $rank(A) \le n$, choosing an appropriate set of weight vectors $\{v_i\}_{1 \le i \le n}$, we have $A = \sum_{i=1}^n u_i v_i^T$.
\end{proof}

\end{document}
