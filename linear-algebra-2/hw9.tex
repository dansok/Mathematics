\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm, mathrsfs}
\addtolength{\evensidemargin}{-.5in}
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\textwidth}{0.8in}
\addtolength{\textheight}{0.8in}
\addtolength{\topmargin}{-.4in}
\newtheoremstyle{quest}{\topsep}{\topsep}{}{}{\bfseries}{}{ }{\thmname{#1}\thmnote{ #3}.}
\theoremstyle{quest}
\newtheorem*{definition}{Definition}
\newtheorem*{theorem}{Theorem}
\newtheorem*{question}{Question}
\newtheorem*{exercise}{Exercise}
\newtheorem*{challengeproblem}{Challenge Problem}
\newcommand{\name}{%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% put your name here, so we know who to give credit to %%
Dan Sokolsky
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\hw}{%%%%%%%%%%%%%%%%%%%%
%% and which homework assignment is it? %%%%%%%%%
%% put the correct number below              %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
9
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\vspace{-50pt}
\Huge \name
\\\vspace{20pt}
\huge Linear Algebra II\hfill Homework \hw}
\author{}
\date{}
\pagestyle{myheadings}
\markright{\name\hfill Homework \hw\qquad\hfill}

%% If you want to define a new command, you can do it like this:
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

%% If you want to use a function like ''sin'' or ''cos'', you can do it like this
%% (we probably won't have much use for this)
% \DeclareMathOperator{\sin}{sin}   %% just an example (it's already defined)


\begin{document}
\maketitle

\begin{question}[1]
\end{question}
\begin{proof}
By Theorem $12(ii)$ on page $87$, we must prove $\\V^TV = I_n = (VQV^T)^T(VQV^T)$. We begin with the second part of the lemma --
$\\(ii)\ (V^TV)_{ij} = \langle v_i, v_j \rangle = \delta_{ij} = (I_n)_{ij} \iff V^TV = I_n$
$\\(i)$ Since $Q$ is orthogonal, we have $Q^TQ = I_n$ by definition.
Since in $(ii)$ we proved $V$ is an isometry, we have $detV = \pm 1$. Thus
$$det(VV^T) = detV \cdot detV^T = (detV)^2 = (\pm 1)^2 = 1 \ne 0$$
Thus $VV^T$ is invertible. Therefore,
$$VV^T \cdot VV^T = V(V^TV)V^T = V \cdot I_n \cdot V^T = VV^T$$
$$\iff VV^T = (VV^T)^{-1} (VV^T \cdot VV^T) = (VV^T)^{-1} (VV^T) = I_n$$Alternatively, $detV^T = detV = \pm 1$ proves $V^T$ is an isometry. Thus $\\VV^T = (V^T)^TV^T = I_n$.
\\Thus, we have
$$(VQV^T)^T(VQV^T) = VQ^TV^TVQV^T = VQ^TQV^T = VV^T = I_n$$
\end{proof}

\begin{question}[2]
\end{question}
\begin{proof}
$\mathbf{(a)}\ (\implies)$ Suppose $A, B$ are isometric. That is, suppose $A = GBH$ with $G, H$ isometric. Then $G, H$ leave norms invariant by question $(3)(b)$ in homework $7$. We have $\sigma_1 = \|A\| = \|GBH\| = \|B\|$. Let us iterate the argument once to see why all the nonzero singular values must be equal by induction. Let $u_1, v_1$ be unit vectors such that $\|Av_1\| = \|A\|$ and $\sigma_1 u_1 = Av_1 = GBHv_1$. Additionally note,
$$\|B\| = \sigma_1 = \|A\| = \|Av_1\| = \|GBHv_1\| = \|B(Hv_1)\|$$
Let $w_1$ be a unit vector such that $\sigma_1 w_1 = B(Hv_1)$. Define
$$A_2 = A - \sigma_1 u_1v_1^T,\ \ B_2 = B - \sigma_1 w_1 (Hv_1)^T$$
Then
$$GB_2H = G(B - \sigma_1 w_1 (Hv_1)^T)H = GBH - G(\sigma_1 w_1 (Hv_1)^T)H = A - G(\sigma_1 w_1v_1^T)H^TH$$
$$= A - G(\sigma_1 w_1v_1^T) \cdot I = A - (GBHv_1)v_1^T = A - Av_1 v_1^T = A - \sigma_1u_1v_1^T = A_2$$
whence $A_2, B_2$ are isomorphic under the same $G, H$; $\sigma_2 = \|A_2\| = \|GB_2H\| = \|B_2\|$, and we can repeat the procedure. We terminate at the first $i$ such that $\|A_i\| = \|B_i\| = 0$, where thereafter all the singular values are $0$. This proves that $A, B$ share their nonzero singular values.
$\\(\impliedby)$ Suppose $A, B$ share their nonzero singular values. Suppose $A \in \mathbb{R}^{m_1 \times n_1}, B \in \mathbb{R}^{m_2 \times n_2}$. WLOG, suppose $m_1 \le m_2, n_1 \le n_2$. Let $B = U_B\Sigma V_B^T$. Let $I_R \in \mathbb{R}^{n_2 \times n_1}$ be a tall identity matrix. Let $I_L \in \mathbb{R}^{m_1 \times m_2}$ be a flat identity matrix. Let $V_A = I_R^T \cdot V_B,\\U_A = U_B \cdot I_L$. Then, either $H^TH$ is an identity, or it is an orthogonal projection. In the latter case, it still behaves like an identity, and so WLOG, will assume $H^TH = I$. Then,
$$A = U_A \Sigma V_A^T = (I_LU_B)\Sigma(I_R^TV_B)^T = I_L (U_B \Sigma V_B^T) I_R = I_L \cdot B \cdot I_R$$
whence $A, B$ are isomorphic since $I_R, I_L^T$ are orthogonal and hence $I_L$ is orthogonal when restricted to $rowspace(A)$.
\\$\mathbf{(b)}$ As mentioned in the comment, $G^TG, HH^T$ are either identities, or they are orthogonal projections (tall/flat identities). In the latter case, all the action beyond the identity does is control the dimensions of the output matrix or project the input matrix orthogonally. Therefore, WLOG, suppose $G^TG = I_m, HH^T = I_n$. Then
$$G^TAH^T = G^T(GBH)H^T = (G^TG)B(HH^T)= I_mB I_n = B$$
\end{proof}

\begin{question}[3]
\end{question}
\begin{proof}
$\mathbf{(a)}$ WLOG, suppose $\|u\| \ge \|v\|$. Then $A^TA = \begin{pmatrix}
\|u\|^2 & 0 \\
0 & \|v\|^2
\end{pmatrix}$. Thus $\\ \lambda_1 = \|u\|^2, \lambda_2 = \|v\|^2$ and $\sigma_1 = \|u\|, \sigma_2 = \|v\|$. Let us find the corresponding eigenvectors, which are the right singular vectors of $A$ --
$$A^TA \beta_1 = \|u\|^2 \beta_1 \iff \beta_1 = \begin{pmatrix}
1 \\
0
\end{pmatrix} $$
$$A^TA \beta_2 = \|v\|^2 \beta_2 \iff \beta_2 = \begin{pmatrix}
0 \\
1
\end{pmatrix}$$
Now the left singular vectors are such that $\sigma_i \alpha_i = A \beta_i$. That is,
$$\|u\| \alpha_1 = \sigma_1 \alpha_1 = A \beta_1 = A \begin{pmatrix}
1 \\
0
\end{pmatrix} = u \iff \alpha_1 = \dfrac{u}{\|u\|}$$
$$\|v\| \alpha_2 = \sigma_2 \alpha_2 = A \beta_2 = A \begin{pmatrix}
0 \\
1
\end{pmatrix} = v \iff \alpha_2 = \dfrac{v}{\|v\|}$$
Thus, the SVD of A is given by --
$$A = \sum_{i=1}^2 \sigma_i \alpha_i \beta_i^T = \|u\| \cdot \dfrac{u}{\|u\|} \cdot \begin{pmatrix}
1 \\
0
\end{pmatrix}^T + \|v\| \cdot \dfrac{v}{\|v\|} \cdot \begin{pmatrix}
0 \\
1
\end{pmatrix}^T$$
That is, we can write --
$$A = \begin{pmatrix}
u & v
\end{pmatrix} = \begin{pmatrix}
u_1 & v_1 \\
u_2 & v_2 \\
\vdots & \vdots \\
u_n & v_n \\
\end{pmatrix} = \begin{pmatrix}
\frac{u_1}{\|u\|} & \frac{v_1}{\|v\|} & 0 & \ldots & 0 \\
\frac{u_2}{\|u\|} & \frac{v_2}{\|v\|} & 0 & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\frac{u_n}{\|u\|} & \frac{v_n}{\|v\|} & 0 & \ldots & 0 \\
\end{pmatrix} \cdot  \begin{pmatrix}
\|u\| & 0 \\
0 & \|v\| \\
0 & 0 \\
\vdots & \vdots \\
0 & 0
\end{pmatrix} \cdot \begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix} = G \cdot \Sigma \cdot H$$
Since $\frac{u}{\|u\|}, \frac{v}{\|v\|}$ are orthonormal, $G$ is an isometry, and $H$ is clearly an isometry as an identity matrix. Thus $A$ is isometric to $\Sigma = \begin{pmatrix}
\|u\| & 0 \\
0 & \|v\| \\
0 & 0 \\
\vdots & \vdots \\
0 & 0
\end{pmatrix}$.
\\$\mathbf{(b)}$ $C^TC = \|u\| \begin{pmatrix}
1 & 1 \\
1 & 1
\end{pmatrix} = \|u\| \cdot M$ where the eigenvalues of $M$ are $2$, and $0$. Let us find the corresponding eigenvector, which is also the right singular vector --
$$\begin{pmatrix}
2x_1 \\
2x_2
\end{pmatrix} = 2 \cdot \begin{pmatrix}
x_1 \\
x_2
\end{pmatrix} = \begin{pmatrix}
1 & 1 \\
1 & 1
\end{pmatrix} \cdot \begin{pmatrix}
x_1 \\
x_2
\end{pmatrix} = \begin{pmatrix}
x_1 + x_2\\
x_1 + x_2
\end{pmatrix}$$
$\iff x_1 = x_2$. Take $\beta_1 = \begin{pmatrix}
\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}}
\end{pmatrix}$ to be the unit eigenvector of $C^TC$ corresponding to $\sigma_1 = 2\|u\|$, and the right singular vector of $C$. Take $\beta_2 = \begin{pmatrix}
-\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}}
\end{pmatrix}$ to be the right unit singular vector corresponding to $0$, and observe $\beta_1 \perp \beta_2$.
Now the left singular vectors are such that $\sigma_i \alpha_i = A \beta_i$. We won't compute the second left singular vector since $\sigma_2 = 0$. That is,
$$2 \|u\| \alpha_1 = \sigma_1 \alpha_1 = A \beta_1 = \begin{pmatrix}
u_1 & u_1 \\
\vdots & \vdots \\
u_n & u_n
\end{pmatrix} \cdot \begin{pmatrix}
\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}}
\end{pmatrix} = \sqrt{2} \cdot u$$
$$\iff \alpha_1 = \dfrac{\sqrt{2}}{2\|u\|} \cdot u$$
Thus
$$A = \sum_{i=1}^2 \sigma_i \alpha_i \beta_i^T = \sigma_1 \alpha_1 \beta_1^T = 2\|u\| \cdot \dfrac{\sqrt{2}}{2 \|u\|} \cdot u \cdot \begin{pmatrix}
\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}}
\end{pmatrix}^T = u \cdot \begin{pmatrix}
1 & 1
\end{pmatrix} $$
That is, we can write --
$$A = \begin{pmatrix}
\frac{\sqrt{2}}{2 \|u\|} u_1 & 0 & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\sqrt{2}}{2 \|u\|} u_n & 0 & \ldots & 0
\end{pmatrix} \cdot \begin{pmatrix}
2\|u\| & 0 \\
0 & 0 \\
\vdots & \vdots \\
0 & 0
\end{pmatrix} \cdot \begin{pmatrix}
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}
\end{pmatrix} =$$ $$\begin{pmatrix}
\frac{u_1}{\|u\|} & 0 & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
\frac{u_n}{\|u\|} & 0 & \ldots & 0
\end{pmatrix} \cdot \begin{pmatrix}
\sqrt{2}\|u\| & 0 \\
0 & 0 \\
\vdots & \vdots \\
0 & 0
\end{pmatrix} \cdot \begin{pmatrix}
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}
\end{pmatrix} = G \cdot \Sigma \cdot H$$
where $G$ is an isometry since it's first column is a unit vector is clearly orthogonal to the rest of the zero-columns, and $H$ is clearly an isometry. Thus $A$ is isometric to $\Sigma = \begin{pmatrix}
\sqrt{2}\|u\| & 0 \\
0 & 0 \\
\vdots & \vdots \\
0 & 0
\end{pmatrix}$.
\\$\mathbf{(c)}$ Let $Q = \begin{pmatrix}
q_1 & \ldots & q_n
\end{pmatrix} $. The singular values are the square roots of the eigenvalues of $Q^TQ$. To that end,
$$(Q^TQ)_{ij} = \langle q_i, q_j \rangle = \|q_i\|^2 \delta_{ij}$$
Thus $\sigma_i = \|q_i\|, 1 \le i \le n$ are the $n$ singular values.
\end{proof}

\begin{question}[4]
\end{question}
\begin{proof}
$\mathbf{(a)}$ Since $B, C$ are real symmetric positive definite, $B^{\frac{1}{2}}, C^{\frac{1}{2}}$ exist and are real symmetric positive definite themselves. Additionally, $B, C, B^{\frac{1}{2}}, C^{\frac{1}{2}}$ are all isometries. Define $\tilde{A} = B^{\frac{1}{2}} A C^{\frac{1}{2}}$. Then $\tilde{A} = \tilde{U}  \tilde{\Sigma} \tilde{V}^T$. Let $U = B^{-\frac{1}{2}} \tilde{U},\ V = C^{-\frac{1}{2}} \tilde{V}$. Then $U, V$ are orthogonal. Since $B^{\frac{1}{2}}, C^{\frac{1}{2}}$ are isometries, it follows that $\|\tilde{A}\| = \|B^{\frac{1}{2}} A C^{\frac{1}{2}}\| = \|A\|$ so all the singular values $\{\sigma_i\}_{1 \le i \le n}$ are invariant under the weighted inner products. Thus $\Sigma = \tilde{\Sigma}$. Finally
$$A = B^{-\frac{1}{2}} \tilde{A} C^{-\frac{1}{2}} = B^{-\frac{1}{2}} \tilde{U}  \tilde{\Sigma} \tilde{V}^T C^{-\frac{1}{2}} = B^{-\frac{1}{2}} \tilde{U}  \Sigma \tilde{V}^T C^{-\frac{1}{2}} = U \Sigma V^T$$
$\mathbf{(b)}$ By $(a)$, we have $A = \sum_j \sigma_j \cdot (B^{-\frac{1}{2}} \tilde{u_j}) \cdot (C^{-\frac{1}{2}} \tilde{v_j})^T$
\end{proof}

\begin{question}[5]
\end{question}
\begin{proof}
Suppose $\{f(s_j)\}_{1 \le j \le m}$ are linearly independent and $f(t) \in span\{f(s_j)\}$ for all $t \ne s_j$ for any $1 \le j \le m$. Then
$$\begin{pmatrix}
f_1(t) \\
\vdots \\
f_n(t)
\end{pmatrix} = f(t) = \sum_{j=1}^m c_j f(s_j) = \begin{pmatrix}
c_1 \cdot f_1(s_1) + \ldots + c_m \cdot f_1(s_m) \\
\vdots \\
c_1 \cdot f_n(s_1) + \ldots + c_m \cdot f_n(s_m)
\end{pmatrix}$$
This system of linear equations can be described by the variable and coefficient matrices, respectively -- 
$$\begin{pmatrix}
c_1 \cdot f_1(s_1) + \ldots + c_m \cdot f_1(s_m) \\
\vdots \\
c_1 \cdot f_n(s_1) + \ldots + c_m \cdot f_n(s_m)
\end{pmatrix} = \begin{pmatrix}
f_1(s_1) & f_1(s_2) & \ldots & f_1(s_m) \\
f_2(s_1) & f_2(s_2) & \ldots & f_2(s_m) \\
\vdots & \vdots & \ddots & \vdots \\
f_n(s_1) & f_n(s_2) & \ldots & f_n(s_m)
\end{pmatrix} \cdot \begin{pmatrix} c_1 & c_1 & \ldots & c_1 \\
c_2 & c_2 & \ldots & c_2 \\
\vdots & \vdots & \ddots & \vdots \\
c_m & c_m & \ldots & c_m
\end{pmatrix}\ \ \ \ \ \ \ (9.1)$$
and the system is overdetermined since $m < n$, 
which implies a nontrivial linear relation between the rows of the system, say $\sum_{j=1}^m c_j \cdot f_i(s_j) = \sum_{k \ne i} a_k \cdot \sum_{j=1}^m b_j \cdot f_k(s_j) \iff \\c_j \cdot f_i(s_j) = \sum_{k \ne i} a_kb_j \cdot f_k(s_j)$. That is, $f_i(s_j) \in span\{f_k(s_j)\}_{k \ne i}$, contradicting the fact that $\{f_i\}_{1 \le i \le n}$ are linearly independent.
\\
\end{proof}

\begin{question}[6]
\end{question}
\begin{proof}
If there are only $m$ linearly independent columns $\{s_j\}_{1 \le j \le m}$ in $f(t)$, then
\\$rank(A^T) = rank(f(t)^T) = rank(f(t)) = rank(A) = m$, meaning there are also only $m$ linearly independent rows in $A = f(t)$. Say the $i^{th}$ row is in the span of the rest of the rows. Then,
$$\begin{pmatrix}
f_i(s_1) \\
\vdots \\
f_i(s_m)
\end{pmatrix} = \sum_{j \ne i} c_j \cdot \begin{pmatrix}
f_j(s_1) \\
\vdots \\
f_j(s_m)
\end{pmatrix} = \begin{pmatrix}
\sum_{j \ne i} c_j \cdot f_j(s_1) \\
\vdots \\
\sum_{j \ne i} c_j \cdot f_j(s_m)
\end{pmatrix}$$
$$\iff f_i(s_k) = \sum_{j \ne i} c_j \cdot f_j(s_k)$$
contradicting the fact that $\{f_i\}_{1 \le i \le n}$ are linearly independent.
\end{proof}

\end{document}
