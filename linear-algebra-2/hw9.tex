\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm, mathrsfs}
\addtolength{\evensidemargin}{-.5in}
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\textwidth}{0.8in}
\addtolength{\textheight}{0.8in}
\addtolength{\topmargin}{-.4in}
\newtheoremstyle{quest}{\topsep}{\topsep}{}{}{\bfseries}{}{ }{\thmname{#1}\thmnote{ #3}.}
\theoremstyle{quest}
\newtheorem*{definition}{Definition}
\newtheorem*{theorem}{Theorem}
\newtheorem*{question}{Question}
\newtheorem*{exercise}{Exercise}
\newtheorem*{challengeproblem}{Challenge Problem}
\newcommand{\name}{%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% put your name here, so we know who to give credit to %%
Dan Sokolsky
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\hw}{%%%%%%%%%%%%%%%%%%%%
%% and which homework assignment is it? %%%%%%%%%
%% put the correct number below              %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
9
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\vspace{-50pt}
\Huge \name
\\\vspace{20pt}
\huge Linear Algebra II\hfill Homework \hw}
\author{}
\date{}
\pagestyle{myheadings}
\markright{\name\hfill Homework \hw\qquad\hfill}

%% If you want to define a new command, you can do it like this:
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

%% If you want to use a function like ''sin'' or ''cos'', you can do it like this
%% (we probably won't have much use for this)
% \DeclareMathOperator{\sin}{sin}   %% just an example (it's already defined)


\begin{document}
\maketitle

\begin{question}[1]
\end{question}
\begin{proof}
By Theorem $12(ii)$ on page $87$, we must prove $\\V^TV = I_n = (VQV^T)^T(VQV^T)$. We begin with the second part of the lemma --
$\\(ii)\ (V^TV)_{ij} = \langle v_i, v_j \rangle = \delta_{ij} = (I_n)_{ij} \iff V^TV = I_n$
$\\(i)$ Since $Q$ is orthogonal, we have $Q^TQ = I_n$ by definition.
Since in $(ii)$ we proved $V$ is an isometry, we have $detV = \pm 1$. Thus
$$det(VV^T) = detV \cdot detV^T = (detV)^2 = (\pm 1)^2 = 1 \ne 0$$
Thus $VV^T$ is invertible. Therefore,
$$VV^T \cdot VV^T = V(V^TV)V^T = V \cdot I_n \cdot V^T = VV^T$$
$$\iff VV^T = (VV^T)^{-1} (VV^T \cdot VV^T) = (VV^T)^{-1} (VV^T) = I_n$$Alternatively, $detV^T = detV = \pm 1$ proves $V^T$ is an isometry. Thus $\\VV^T = (V^T)^TV^T = I_n$.
\\Thus, we have
$$(VQV^T)^T(VQV^T) = VQ^TV^TVQV^T = VQ^TQV^T = VV^T = I_n$$
\end{proof}

\begin{question}[2]
\end{question}
\begin{proof}
$\mathbf{(a)}\ (\implies)$ Suppose $A, B$ are isometric. That is, suppose $A = GBH$ with $G, H$ isometric. Then $G, H$ leave norms invariant by question $(3)(b)$ in homework $7$. We have $\sigma_1 = \|A\| = \|GBH\| = \|B\|$. Let us iterate the argument once to see why all the nonzero singular values must be equal by induction. Let $u_1, v_1$ be unit vectors such that $\|Av_1\| = \|A\|$ and $\sigma_1 u_1 = Av_1 = GBHv_1$. Additionally note,
$$\|B\| = \sigma_1 = \|A\| = \|Av_1\| = \|GBHv_1\| = \|B(Hv_1)\|$$
Let $w_1$ be a unit vector such that $\sigma_1 w_1 = B(Hv_1)$. Define
$$A_2 = A - \sigma_1 u_1v_1^T,\ \ B_2 = B - \sigma_1 w_1 (Hv_1)^T$$
Then
$$GB_2H = G(B - \sigma_1 w_1 (Hv_1)^T)H = GBH - G(\sigma_1 w_1 (Hv_1)^T)H = A - G(\sigma_1 w_1v_1^T)H^TH$$
$$= A - G(\sigma_1 w_1v_1^T) \cdot I = A - (GBHv_1)v_1^T = A - Av_1 v_1^T = A - \sigma_1u_1v_1^T = A_2$$
whence $A_2, B_2$ are isomorphic under the same $G, H$; $\sigma_2 = \|A_2\| = \|GB_2H\| = \|B_2\|$, and we can repeat the procedure. We terminate at the first $i$ such that $\|A_i\| = \|B_i\| = 0$, where thereafter all the singular values are $0$. This proves that $A, B$ share their nonzero singular values.
$\\(\impliedby)$ Suppose $A, B$ share their nonzero singular values. Suppose $A \in \mathbb{R}^{m_1 \times n_1}, B \in \mathbb{R}^{m_2 \times n_2}$. WLOG, suppose $m_1 \le m_2, n_1 \le n_2$. Let $B = U_B\Sigma V_B^T$. Let $I_R \in \mathbb{R}^{n_2 \times n_1}$ be a tall identity matrix. Let $I_L \in \mathbb{R}^{m_1 \times m_2}$ be a flat identity matrix. Let $V_A = I_R^T \cdot V_B,\\U_A = U_B \cdot I_L$. Then, either $H^TH$ is an identity, or it is an orthogonal projection. In the latter case, it still behaves like an identity, and so WLOG, will assume $H^TH = I$. Then,
$$A = U_A \Sigma V_A^T = (I_LU_B)\Sigma(I_R^TV_B)^T = I_L (U_B \Sigma V_B^T) I_R = I_L \cdot B \cdot I_R$$
whence $A, B$ are isomorphic since $I_R, I_L^T$ are orthogonal and hence $I_L$ is orthogonal when restricted to $rowspace(A)$.
\\$\mathbf{(b)}$ As mentioned in the comment, $G^TG, HH^T$ are either identities, or they are orthogonal projections (tall/flat identities). In the latter case, all the action beyond the identity does is control the dimensions of the output matrix or project the input matrix orthogonally. Therefore, WLOG, suppose $G^TG = I_m, HH^T = I_n$. Then
$$G^TAH^T = G^T(GBH)H^T = (G^TG)B(HH^T)= I_mB I_n = B$$
\end{proof}

\begin{question}[3]
\end{question}
\begin{proof}
$\mathbf{(a)}$ WLOG, suppose $\|u\| \ge \|v\|$. Then $A^TA = \begin{pmatrix}
\|u\|^2 & 0 \\
0 & \|v\|^2
\end{pmatrix}$. Thus $\\ \lambda_1 = \|u\|^2, \lambda_2 = \|v\|^2$ and $\sigma_1 = \|u\|, \sigma_2 = \|v\|$. Let us find the corresponding eigenvectors, which are the right singular vectors of $A$ --
$$A^TA \beta_1 = \|u\|^2 \beta_1 \iff \beta_1 = \begin{pmatrix}
1 \\
0
\end{pmatrix} $$
$$A^TA \beta_2 = \|v\|^2 \beta_2 \iff \beta_2 = \begin{pmatrix}
0 \\
1
\end{pmatrix}$$
Now the left singular vectors are such that $\sigma_i \alpha_i = A \beta_i$. That is,
$$\|u\| \alpha_1 = \sigma_1 \alpha_1 = A \beta_1 = A \begin{pmatrix}
1 \\
0
\end{pmatrix} = u \iff \alpha_1 = \dfrac{u}{\|u\|}$$
$$\|v\| \alpha_2 = \sigma_2 \alpha_2 = A \beta_2 = A \begin{pmatrix}
0 \\
1
\end{pmatrix} = v \iff \alpha_2 = \dfrac{v}{\|v\|}$$
Thus, the SVD of A is given by --
$$A = \sum_{i=1}^2 \sigma_i \alpha_i \beta_i^T = \|u\| \cdot \dfrac{u}{\|u\|} \cdot \begin{pmatrix}
1 \\
0
\end{pmatrix}^T + \|v\| \cdot \dfrac{v}{\|v\|} \cdot \begin{pmatrix}
0 \\
1
\end{pmatrix}^T$$
That is, we can write --
$$A = \begin{pmatrix}
u & v
\end{pmatrix} = \begin{pmatrix}
u_1 & v_1 \\
u_2 & v_2 \\
\vdots & \vdots \\
u_n & v_n \\
\end{pmatrix} = \begin{pmatrix}
\frac{u_1}{\|u\|} & \frac{v_1}{\|v\|} & 0 & \ldots & 0 \\
\frac{u_2}{\|u\|} & \frac{v_2}{\|v\|} & 0 & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\frac{u_n}{\|u\|} & \frac{v_n}{\|v\|} & 0 & \ldots & 0 \\
\end{pmatrix} \cdot  \begin{pmatrix}
\|u\| & 0 \\
0 & \|v\| \\
0 & 0 \\
\vdots & \vdots \\
0 & 0
\end{pmatrix} \cdot \begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix} = G \cdot \Sigma \cdot H$$
Since $\frac{u}{\|u\|}, \frac{v}{\|v\|}$ are orthonormal, $G$ is an isometry when restricted to $colspace(\Sigma)$, and $H$ is clearly an isometry as an identity matrix. Thus $A$ is isometric to $\Sigma = \begin{pmatrix}
\|u\| & 0 \\
0 & \|v\| \\
0 & 0 \\
\vdots & \vdots \\
0 & 0
\end{pmatrix}$.
\\$\mathbf{(b)}$ $C^TC = \|u\| \begin{pmatrix}
1 & 1 \\
1 & 1
\end{pmatrix} = \|u\| \cdot M$ where the eigenvalues of $M$ are $2$, and $0$. Let us find the corresponding eigenvector, which is also the right singular vector --
$$\begin{pmatrix}
2x_1 \\
2x_2
\end{pmatrix} = 2 \cdot \begin{pmatrix}
x_1 \\
x_2
\end{pmatrix} = \begin{pmatrix}
1 & 1 \\
1 & 1
\end{pmatrix} \cdot \begin{pmatrix}
x_1 \\
x_2
\end{pmatrix} = \begin{pmatrix}
x_1 + x_2\\
x_1 + x_2
\end{pmatrix}$$
$\iff x_1 = x_2$. Take $\beta_1 = \begin{pmatrix}
\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}}
\end{pmatrix}$ to be the unit eigenvector of $C^TC$ corresponding to $\sigma_1 = \sqrt{2\|u\|}$, and the right singular vector of $C$. Take $\beta_2 = \begin{pmatrix}
-\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}}
\end{pmatrix}$ to be the right unit singular vector corresponding to $0$, and observe $\beta_1 \perp \beta_2$.
Now the left singular vectors are such that $\sigma_i \alpha_i = A \beta_i$. We won't compute the second left singular vector since $\sigma_2 = 0$. That is,
$$\sqrt{2\|u\|} \alpha_1 = \sigma_1 \alpha_1 = A \beta_1 = \begin{pmatrix}
u_1 & u_1 \\
\vdots & \vdots \\
u_n & u_n
\end{pmatrix} \cdot \begin{pmatrix}
\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}}
\end{pmatrix} = \sqrt{2} \cdot u$$
$$\iff \alpha_1 = \dfrac{u}{\sqrt{\|u\|}}$$
Thus
$$A = \sum_{i=1}^2 \sigma_i \alpha_i \beta_i^T = \sigma_1 \alpha_1 \beta_1^T = \sqrt{2\|u\|} \cdot \dfrac{u}{\sqrt{\|u\|}} \cdot \begin{pmatrix}
\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}}
\end{pmatrix}^T = u \cdot \begin{pmatrix}
1 & 1
\end{pmatrix} $$
That is, we can write --
$$A = \begin{pmatrix}
\frac{u_1}{\sqrt{\|u\|}} & 0 & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
\frac{u_n}{\sqrt{\|u\|}} & 0 & \ldots & 0
\end{pmatrix} \cdot \begin{pmatrix}
\sqrt{2\|u\|} & 0 \\
0 & 0 \\
\vdots & \vdots \\
0 & 0
\end{pmatrix} \cdot \begin{pmatrix}
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}
\end{pmatrix} =$$ $$\begin{pmatrix}
\frac{u_1}{\|u\|} & 0 & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
\frac{u_n}{\|u\|} & 0 & \ldots & 0
\end{pmatrix} \cdot \begin{pmatrix}
\sqrt{2}\|u\| & 0 \\
0 & 0 \\
\vdots & \vdots \\
0 & 0
\end{pmatrix} \cdot \begin{pmatrix}
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}
\end{pmatrix} = G \cdot \Sigma \cdot H$$
where $G$ is an isometry when restricted to $colspace(\Sigma)$ since it's first column is a unit vector and is clearly orthogonal to the rest of the zero-columns, and $H$ is clearly an isometry. Thus $A$ is isometric to $\Sigma = \begin{pmatrix}
\sqrt{2}\|u\| & 0 \\
0 & 0 \\
\vdots & \vdots \\
0 & 0
\end{pmatrix}$.
\\$\mathbf{(c)}$ Let $Q = \begin{pmatrix}
q_1 & \ldots & q_n
\end{pmatrix} $. The singular values are the square roots of the eigenvalues of $Q^TQ$. To that end,
$$(Q^TQ)_{ij} = \langle q_i, q_j \rangle = \|q_i\|^2 \delta_{ij}$$
Thus $\sigma_i = \|q_i\|, 1 \le i \le n$ are the $n$ singular values.
\end{proof}

\begin{question}[4]
\end{question}
\begin{proof}
Let $F = span\{f_1, \ldots, f_m\}$, and let $G = span\{f_{m+1}, \ldots, f_n\}$. Observe that for $f \in F, g \in G,\ f \cdot g$ is an odd function. Then,
$$\langle f, g \rangle = \int_{-1}^{1} f(x) \cdot g(x) dx = 0$$
Thus $F \perp G$, whence we can write $F \oplus G = X$, and by question $(3)$ on homework $10$, we can recover $SVD(A)$ from $SVD(A \restriction_F), SVD(A \restriction_G)$ as follows --
$$A = \begin{pmatrix}
A \restriction_F & 0 \\
0 & A \restriction_G
\end{pmatrix}  = \begin{pmatrix}
U_F \cdot \Sigma_F \cdot V_F^T & 0 \\
0 & U_G \cdot \Sigma_G \cdot V_G^T
\end{pmatrix} =$$
$$\begin{pmatrix}
U_F& 0 \\
0 & U_G
\end{pmatrix} \cdot \begin{pmatrix}
\Sigma_F& 0 \\
0 & \Sigma_G
\end{pmatrix} \cdot \begin{pmatrix}
V_F^T& 0 \\
0 & V_G^T
\end{pmatrix} = U \cdot \Sigma \cdot V^T$$
where $U, V$ are unitary and are therefore isometries, and $\Sigma$ is the desired block diagonal matrix for which $A, \Sigma$ are isometric.
\end{proof}

\begin{question}[5]
Note to the reader: We write out the blocks throughout the proof (we refer to it from here on out as "index notation") purely as a reading aide for the reader (and the author) to help visualize the situation. We don't take advantage of the index notation whatsoever, and proceed to use block notation as far as the content of the proof is concerned.
\end{question}
\begin{proof}
WLOG, suppose the first $k$ columns of $A$ are the linearly independent. Reordering columns certainly doesn't alter the rank. All it does is shuffle the basis elements with respect to which the matrix $A$ is defined.
\\Let the $n \times k,\ B = \begin{pmatrix}
a_1 & \ldots & a_k
\end{pmatrix}$ be the matrix of the first $k$ linearly independent columns in $A$. Let the $k \times n,\ C = \begin{pmatrix}
1 & 0 & \ldots & 0 & c_{1,k+1} & \ldots & c_{1n} \\
0 & 1 & \ldots & 0 & c_{2,k+1} & \ldots & c_{2n} \\
\vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & \ldots & 1 & c_{n,k+1} & \ldots & c_{kn}
\end{pmatrix} $ be a matrix of coefficients such that
$$A = B \cdot C$$
Then $C$ clearly has $k$ linearly independent rows due to $I_k$ in the front, and the rows of $A$ are linear combinations of the rows of $C$. In particular, where $A_i, C_j$ are row vectors, we have --
$$A_i = \sum_{j=1}^k a_{ij} C_j$$
That is, $A$ may have at most $k$ linearly independent rows.
\\I.e., $\textit{rowrank}(A) \le \textit{colrank(A)} = k$.
\\We may apply the above argument to $A^T$, yielding --
$$k=\textit{colrank}(A) = \textit{rowrank}(A^T) \le \textit{colrank}(A^T) = \textit{rowrank}(A)$$
That is, $k \le \textit{rowrank}(A) \le k \iff \textit{rowrank}(A) = k$.
\\In other words, $A$ has $k$ linearly independent rows as well.
\end{proof}

\end{document}
